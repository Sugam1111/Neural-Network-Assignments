{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perceptron\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'divide': 'warn', 'over': 'warn', 'under': 'ignore', 'invalid': 'warn'}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.seterr('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perceptron Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here I am building a normal forward propogation for a perceptron layer\n",
    "# and the various function i would use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################\n",
    "## Declaring Training Data        ############\n",
    "#############################################\n",
    "X_train = np.array([[0,0],[1,0],[0,1],[1,1]])\n",
    "Y_train = np.array([[1],[0],[0],[0]])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from MultiLayerPerceptron import MultiLayerPerceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 1) (1,)\n",
      "<function sigmoid at 0x7fb06d3c82f0>\n"
     ]
    }
   ],
   "source": [
    "# Declare a neuron with shape of weights as [shape_of_input,1]\n",
    "model = MultiLayerPerceptron([2,1],['sigmoid'])\n",
    "# print(model.layers[1].W)\n",
    "# print(model.layers[1].b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing the model outputs\n",
    "pred , _ = model.forward(X_train)\n",
    "# print(np.sum((pred > 0.5)== Y_train) / Y_train.shape[0])\n",
    "# accuracy(model , X_train,Y_train)\n",
    "# X_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The loss is [0.25998501]\n",
      "And the error to be back propogated is:\n",
      " [[-0.125     ]\n",
      " [ 0.10798757]\n",
      " [ 0.22948564]\n",
      " [ 0.22370188]]\n"
     ]
    }
   ],
   "source": [
    "# Checking for testing purposes(BCE should be used here ideally)\n",
    "from Loss import mean_abs_error,mean_square_error\n",
    "loss,d_back = mean_square_error(pred,Y_train)\n",
    "print(\"The loss is {}\\nAnd the error to be back propogated is:\\n {}\".format(loss , d_back))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The loss is [6.0109929]\n",
      "And the error to be back propogated is:\n",
      " [[-2.        ]\n",
      " [ 1.76040923]\n",
      " [12.18658652]\n",
      " [ 9.50638367]]\n"
     ]
    }
   ],
   "source": [
    "from Loss import binary_cross_entropy,mean_binary_cross_entropy\n",
    "loss,d_back = binary_cross_entropy(pred,Y_train)\n",
    "print(\"The loss is {}\\nAnd the error to be back propogated is:\\n {}\".format(loss , d_back))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 3) (3,)\n",
      "<function sigmoid at 0x7fb06d3c82f0>\n",
      "(3, 1) (1,)\n",
      "<function sigmoid at 0x7fb06d3c82f0>\n",
      "0th EPOCH:\n",
      "Training Loss:[0.59932499]|Training Accuracy:0.75|Test Loss:[0.59524835]|Test Accuracy:0.75\n",
      "1th EPOCH:\n",
      "Training Loss:[0.59524835]|Training Accuracy:0.75|Test Loss:[0.59122565]|Test Accuracy:0.75\n",
      "2th EPOCH:\n",
      "Training Loss:[0.59122565]|Training Accuracy:0.75|Test Loss:[0.58725448]|Test Accuracy:0.75\n",
      "3th EPOCH:\n",
      "Training Loss:[0.58725448]|Training Accuracy:0.75|Test Loss:[0.58333249]|Test Accuracy:0.75\n",
      "4th EPOCH:\n",
      "Training Loss:[0.58333249]|Training Accuracy:0.75|Test Loss:[0.57945741]|Test Accuracy:0.75\n",
      "5th EPOCH:\n",
      "Training Loss:[0.57945741]|Training Accuracy:0.75|Test Loss:[0.57562707]|Test Accuracy:0.75\n",
      "6th EPOCH:\n",
      "Training Loss:[0.57562707]|Training Accuracy:0.75|Test Loss:[0.57183935]|Test Accuracy:0.75\n",
      "7th EPOCH:\n",
      "Training Loss:[0.57183935]|Training Accuracy:0.75|Test Loss:[0.56809222]|Test Accuracy:0.75\n",
      "8th EPOCH:\n",
      "Training Loss:[0.56809222]|Training Accuracy:0.75|Test Loss:[0.56438373]|Test Accuracy:0.75\n",
      "9th EPOCH:\n",
      "Training Loss:[0.56438373]|Training Accuracy:0.75|Test Loss:[0.56071199]|Test Accuracy:0.75\n",
      "10th EPOCH:\n",
      "Training Loss:[0.56071199]|Training Accuracy:0.75|Test Loss:[0.55707521]|Test Accuracy:0.75\n",
      "11th EPOCH:\n",
      "Training Loss:[0.55707521]|Training Accuracy:0.75|Test Loss:[0.55347163]|Test Accuracy:0.75\n",
      "12th EPOCH:\n",
      "Training Loss:[0.55347163]|Training Accuracy:0.75|Test Loss:[0.54989961]|Test Accuracy:0.75\n",
      "13th EPOCH:\n",
      "Training Loss:[0.54989961]|Training Accuracy:0.75|Test Loss:[0.54635753]|Test Accuracy:0.75\n",
      "14th EPOCH:\n",
      "Training Loss:[0.54635753]|Training Accuracy:0.75|Test Loss:[0.5428439]|Test Accuracy:0.75\n",
      "15th EPOCH:\n",
      "Training Loss:[0.5428439]|Training Accuracy:0.75|Test Loss:[0.53935723]|Test Accuracy:0.75\n",
      "16th EPOCH:\n",
      "Training Loss:[0.53935723]|Training Accuracy:0.75|Test Loss:[0.53589616]|Test Accuracy:0.75\n",
      "17th EPOCH:\n",
      "Training Loss:[0.53589616]|Training Accuracy:0.75|Test Loss:[0.53245936]|Test Accuracy:0.75\n",
      "18th EPOCH:\n",
      "Training Loss:[0.53245936]|Training Accuracy:0.75|Test Loss:[0.52904558]|Test Accuracy:0.75\n",
      "19th EPOCH:\n",
      "Training Loss:[0.52904558]|Training Accuracy:0.75|Test Loss:[0.52565363]|Test Accuracy:0.75\n",
      "20th EPOCH:\n",
      "Training Loss:[0.52565363]|Training Accuracy:0.75|Test Loss:[0.52228239]|Test Accuracy:0.75\n",
      "21th EPOCH:\n",
      "Training Loss:[0.52228239]|Training Accuracy:0.75|Test Loss:[0.51893079]|Test Accuracy:0.75\n",
      "22th EPOCH:\n",
      "Training Loss:[0.51893079]|Training Accuracy:0.75|Test Loss:[0.51559784]|Test Accuracy:0.75\n",
      "23th EPOCH:\n",
      "Training Loss:[0.51559784]|Training Accuracy:0.75|Test Loss:[0.5122826]|Test Accuracy:0.75\n",
      "24th EPOCH:\n",
      "Training Loss:[0.5122826]|Training Accuracy:0.75|Test Loss:[0.50898419]|Test Accuracy:0.75\n",
      "25th EPOCH:\n",
      "Training Loss:[0.50898419]|Training Accuracy:0.75|Test Loss:[0.5057018]|Test Accuracy:0.75\n",
      "26th EPOCH:\n",
      "Training Loss:[0.5057018]|Training Accuracy:0.75|Test Loss:[0.50243466]|Test Accuracy:0.75\n",
      "27th EPOCH:\n",
      "Training Loss:[0.50243466]|Training Accuracy:0.75|Test Loss:[0.49918207]|Test Accuracy:0.75\n",
      "28th EPOCH:\n",
      "Training Loss:[0.49918207]|Training Accuracy:0.75|Test Loss:[0.49594338]|Test Accuracy:1.0\n",
      "29th EPOCH:\n",
      "Training Loss:[0.49594338]|Training Accuracy:1.0|Test Loss:[0.49271798]|Test Accuracy:1.0\n",
      "30th EPOCH:\n",
      "Training Loss:[0.49271798]|Training Accuracy:1.0|Test Loss:[0.48950534]|Test Accuracy:1.0\n",
      "31th EPOCH:\n",
      "Training Loss:[0.48950534]|Training Accuracy:1.0|Test Loss:[0.48630496]|Test Accuracy:1.0\n",
      "32th EPOCH:\n",
      "Training Loss:[0.48630496]|Training Accuracy:1.0|Test Loss:[0.48311639]|Test Accuracy:1.0\n",
      "33th EPOCH:\n",
      "Training Loss:[0.48311639]|Training Accuracy:1.0|Test Loss:[0.47993924]|Test Accuracy:1.0\n",
      "34th EPOCH:\n",
      "Training Loss:[0.47993924]|Training Accuracy:1.0|Test Loss:[0.47677314]|Test Accuracy:1.0\n",
      "35th EPOCH:\n",
      "Training Loss:[0.47677314]|Training Accuracy:1.0|Test Loss:[0.47361779]|Test Accuracy:1.0\n",
      "36th EPOCH:\n",
      "Training Loss:[0.47361779]|Training Accuracy:1.0|Test Loss:[0.47047292]|Test Accuracy:1.0\n",
      "37th EPOCH:\n",
      "Training Loss:[0.47047292]|Training Accuracy:1.0|Test Loss:[0.4673383]|Test Accuracy:1.0\n",
      "38th EPOCH:\n",
      "Training Loss:[0.4673383]|Training Accuracy:1.0|Test Loss:[0.46421374]|Test Accuracy:1.0\n",
      "39th EPOCH:\n",
      "Training Loss:[0.46421374]|Training Accuracy:1.0|Test Loss:[0.46109909]|Test Accuracy:1.0\n",
      "40th EPOCH:\n",
      "Training Loss:[0.46109909]|Training Accuracy:1.0|Test Loss:[0.45799421]|Test Accuracy:1.0\n",
      "41th EPOCH:\n",
      "Training Loss:[0.45799421]|Training Accuracy:1.0|Test Loss:[0.45489903]|Test Accuracy:1.0\n",
      "42th EPOCH:\n",
      "Training Loss:[0.45489903]|Training Accuracy:1.0|Test Loss:[0.4518135]|Test Accuracy:1.0\n",
      "43th EPOCH:\n",
      "Training Loss:[0.4518135]|Training Accuracy:1.0|Test Loss:[0.44873757]|Test Accuracy:1.0\n",
      "44th EPOCH:\n",
      "Training Loss:[0.44873757]|Training Accuracy:1.0|Test Loss:[0.44567125]|Test Accuracy:1.0\n",
      "45th EPOCH:\n",
      "Training Loss:[0.44567125]|Training Accuracy:1.0|Test Loss:[0.44261456]|Test Accuracy:1.0\n",
      "46th EPOCH:\n",
      "Training Loss:[0.44261456]|Training Accuracy:1.0|Test Loss:[0.43956755]|Test Accuracy:1.0\n",
      "47th EPOCH:\n",
      "Training Loss:[0.43956755]|Training Accuracy:1.0|Test Loss:[0.43653028]|Test Accuracy:1.0\n",
      "48th EPOCH:\n",
      "Training Loss:[0.43653028]|Training Accuracy:1.0|Test Loss:[0.43350285]|Test Accuracy:1.0\n",
      "49th EPOCH:\n",
      "Training Loss:[0.43350285]|Training Accuracy:1.0|Test Loss:[0.43048535]|Test Accuracy:1.0\n",
      "50th EPOCH:\n",
      "Training Loss:[0.43048535]|Training Accuracy:1.0|Test Loss:[0.4274779]|Test Accuracy:1.0\n",
      "51th EPOCH:\n",
      "Training Loss:[0.4274779]|Training Accuracy:1.0|Test Loss:[0.42448065]|Test Accuracy:1.0\n",
      "52th EPOCH:\n",
      "Training Loss:[0.42448065]|Training Accuracy:1.0|Test Loss:[0.42149374]|Test Accuracy:1.0\n",
      "53th EPOCH:\n",
      "Training Loss:[0.42149374]|Training Accuracy:1.0|Test Loss:[0.41851731]|Test Accuracy:1.0\n",
      "54th EPOCH:\n",
      "Training Loss:[0.41851731]|Training Accuracy:1.0|Test Loss:[0.41555156]|Test Accuracy:1.0\n",
      "55th EPOCH:\n",
      "Training Loss:[0.41555156]|Training Accuracy:1.0|Test Loss:[0.41259663]|Test Accuracy:1.0\n",
      "56th EPOCH:\n",
      "Training Loss:[0.41259663]|Training Accuracy:1.0|Test Loss:[0.40965273]|Test Accuracy:1.0\n",
      "57th EPOCH:\n",
      "Training Loss:[0.40965273]|Training Accuracy:1.0|Test Loss:[0.40672003]|Test Accuracy:1.0\n",
      "58th EPOCH:\n",
      "Training Loss:[0.40672003]|Training Accuracy:1.0|Test Loss:[0.40379872]|Test Accuracy:1.0\n",
      "59th EPOCH:\n",
      "Training Loss:[0.40379872]|Training Accuracy:1.0|Test Loss:[0.400889]|Test Accuracy:1.0\n",
      "60th EPOCH:\n",
      "Training Loss:[0.400889]|Training Accuracy:1.0|Test Loss:[0.39799106]|Test Accuracy:1.0\n",
      "61th EPOCH:\n",
      "Training Loss:[0.39799106]|Training Accuracy:1.0|Test Loss:[0.39510511]|Test Accuracy:1.0\n",
      "62th EPOCH:\n",
      "Training Loss:[0.39510511]|Training Accuracy:1.0|Test Loss:[0.39223132]|Test Accuracy:1.0\n",
      "63th EPOCH:\n",
      "Training Loss:[0.39223132]|Training Accuracy:1.0|Test Loss:[0.3893699]|Test Accuracy:1.0\n",
      "64th EPOCH:\n",
      "Training Loss:[0.3893699]|Training Accuracy:1.0|Test Loss:[0.38652104]|Test Accuracy:1.0\n",
      "65th EPOCH:\n",
      "Training Loss:[0.38652104]|Training Accuracy:1.0|Test Loss:[0.38368492]|Test Accuracy:1.0\n",
      "66th EPOCH:\n",
      "Training Loss:[0.38368492]|Training Accuracy:1.0|Test Loss:[0.38086173]|Test Accuracy:1.0\n",
      "67th EPOCH:\n",
      "Training Loss:[0.38086173]|Training Accuracy:1.0|Test Loss:[0.37805165]|Test Accuracy:1.0\n",
      "68th EPOCH:\n",
      "Training Loss:[0.37805165]|Training Accuracy:1.0|Test Loss:[0.37525486]|Test Accuracy:1.0\n",
      "69th EPOCH:\n",
      "Training Loss:[0.37525486]|Training Accuracy:1.0|Test Loss:[0.37247153]|Test Accuracy:1.0\n",
      "70th EPOCH:\n",
      "Training Loss:[0.37247153]|Training Accuracy:1.0|Test Loss:[0.36970183]|Test Accuracy:1.0\n",
      "71th EPOCH:\n",
      "Training Loss:[0.36970183]|Training Accuracy:1.0|Test Loss:[0.3669459]|Test Accuracy:1.0\n",
      "72th EPOCH:\n",
      "Training Loss:[0.3669459]|Training Accuracy:1.0|Test Loss:[0.36420392]|Test Accuracy:1.0\n",
      "73th EPOCH:\n",
      "Training Loss:[0.36420392]|Training Accuracy:1.0|Test Loss:[0.36147601]|Test Accuracy:1.0\n",
      "74th EPOCH:\n",
      "Training Loss:[0.36147601]|Training Accuracy:1.0|Test Loss:[0.35876234]|Test Accuracy:1.0\n",
      "75th EPOCH:\n",
      "Training Loss:[0.35876234]|Training Accuracy:1.0|Test Loss:[0.35606302]|Test Accuracy:1.0\n",
      "76th EPOCH:\n",
      "Training Loss:[0.35606302]|Training Accuracy:1.0|Test Loss:[0.35337818]|Test Accuracy:1.0\n",
      "77th EPOCH:\n",
      "Training Loss:[0.35337818]|Training Accuracy:1.0|Test Loss:[0.35070796]|Test Accuracy:1.0\n",
      "78th EPOCH:\n",
      "Training Loss:[0.35070796]|Training Accuracy:1.0|Test Loss:[0.34805245]|Test Accuracy:1.0\n",
      "79th EPOCH:\n",
      "Training Loss:[0.34805245]|Training Accuracy:1.0|Test Loss:[0.34541177]|Test Accuracy:1.0\n",
      "80th EPOCH:\n",
      "Training Loss:[0.34541177]|Training Accuracy:1.0|Test Loss:[0.34278602]|Test Accuracy:1.0\n",
      "81th EPOCH:\n",
      "Training Loss:[0.34278602]|Training Accuracy:1.0|Test Loss:[0.34017529]|Test Accuracy:1.0\n",
      "82th EPOCH:\n",
      "Training Loss:[0.34017529]|Training Accuracy:1.0|Test Loss:[0.33757967]|Test Accuracy:1.0\n",
      "83th EPOCH:\n",
      "Training Loss:[0.33757967]|Training Accuracy:1.0|Test Loss:[0.33499924]|Test Accuracy:1.0\n",
      "84th EPOCH:\n",
      "Training Loss:[0.33499924]|Training Accuracy:1.0|Test Loss:[0.33243408]|Test Accuracy:1.0\n",
      "85th EPOCH:\n",
      "Training Loss:[0.33243408]|Training Accuracy:1.0|Test Loss:[0.32988425]|Test Accuracy:1.0\n",
      "86th EPOCH:\n",
      "Training Loss:[0.32988425]|Training Accuracy:1.0|Test Loss:[0.32734982]|Test Accuracy:1.0\n",
      "87th EPOCH:\n",
      "Training Loss:[0.32734982]|Training Accuracy:1.0|Test Loss:[0.32483084]|Test Accuracy:1.0\n",
      "88th EPOCH:\n",
      "Training Loss:[0.32483084]|Training Accuracy:1.0|Test Loss:[0.32232738]|Test Accuracy:1.0\n",
      "89th EPOCH:\n",
      "Training Loss:[0.32232738]|Training Accuracy:1.0|Test Loss:[0.31983946]|Test Accuracy:1.0\n",
      "90th EPOCH:\n",
      "Training Loss:[0.31983946]|Training Accuracy:1.0|Test Loss:[0.31736715]|Test Accuracy:1.0\n",
      "91th EPOCH:\n",
      "Training Loss:[0.31736715]|Training Accuracy:1.0|Test Loss:[0.31491047]|Test Accuracy:1.0\n",
      "92th EPOCH:\n",
      "Training Loss:[0.31491047]|Training Accuracy:1.0|Test Loss:[0.31246946]|Test Accuracy:1.0\n",
      "93th EPOCH:\n",
      "Training Loss:[0.31246946]|Training Accuracy:1.0|Test Loss:[0.31004415]|Test Accuracy:1.0\n",
      "94th EPOCH:\n",
      "Training Loss:[0.31004415]|Training Accuracy:1.0|Test Loss:[0.30763456]|Test Accuracy:1.0\n",
      "95th EPOCH:\n",
      "Training Loss:[0.30763456]|Training Accuracy:1.0|Test Loss:[0.30524071]|Test Accuracy:1.0\n",
      "96th EPOCH:\n",
      "Training Loss:[0.30524071]|Training Accuracy:1.0|Test Loss:[0.30286262]|Test Accuracy:1.0\n",
      "97th EPOCH:\n",
      "Training Loss:[0.30286262]|Training Accuracy:1.0|Test Loss:[0.3005003]|Test Accuracy:1.0\n",
      "98th EPOCH:\n",
      "Training Loss:[0.3005003]|Training Accuracy:1.0|Test Loss:[0.29815377]|Test Accuracy:1.0\n",
      "99th EPOCH:\n",
      "Training Loss:[0.29815377]|Training Accuracy:1.0|Test Loss:[0.29582303]|Test Accuracy:1.0\n",
      "100th EPOCH:\n",
      "Training Loss:[0.29582303]|Training Accuracy:1.0|Test Loss:[0.29350809]|Test Accuracy:1.0\n",
      "101th EPOCH:\n",
      "Training Loss:[0.29350809]|Training Accuracy:1.0|Test Loss:[0.29120894]|Test Accuracy:1.0\n",
      "102th EPOCH:\n",
      "Training Loss:[0.29120894]|Training Accuracy:1.0|Test Loss:[0.28892559]|Test Accuracy:1.0\n",
      "103th EPOCH:\n",
      "Training Loss:[0.28892559]|Training Accuracy:1.0|Test Loss:[0.28665803]|Test Accuracy:1.0\n",
      "104th EPOCH:\n",
      "Training Loss:[0.28665803]|Training Accuracy:1.0|Test Loss:[0.28440625]|Test Accuracy:1.0\n",
      "105th EPOCH:\n",
      "Training Loss:[0.28440625]|Training Accuracy:1.0|Test Loss:[0.28217026]|Test Accuracy:1.0\n",
      "106th EPOCH:\n",
      "Training Loss:[0.28217026]|Training Accuracy:1.0|Test Loss:[0.27995003]|Test Accuracy:1.0\n",
      "107th EPOCH:\n",
      "Training Loss:[0.27995003]|Training Accuracy:1.0|Test Loss:[0.27774555]|Test Accuracy:1.0\n",
      "108th EPOCH:\n",
      "Training Loss:[0.27774555]|Training Accuracy:1.0|Test Loss:[0.27555682]|Test Accuracy:1.0\n",
      "109th EPOCH:\n",
      "Training Loss:[0.27555682]|Training Accuracy:1.0|Test Loss:[0.27338381]|Test Accuracy:1.0\n",
      "110th EPOCH:\n",
      "Training Loss:[0.27338381]|Training Accuracy:1.0|Test Loss:[0.2712265]|Test Accuracy:1.0\n",
      "111th EPOCH:\n",
      "Training Loss:[0.2712265]|Training Accuracy:1.0|Test Loss:[0.26908488]|Test Accuracy:1.0\n",
      "112th EPOCH:\n",
      "Training Loss:[0.26908488]|Training Accuracy:1.0|Test Loss:[0.26695893]|Test Accuracy:1.0\n",
      "113th EPOCH:\n",
      "Training Loss:[0.26695893]|Training Accuracy:1.0|Test Loss:[0.26484862]|Test Accuracy:1.0\n",
      "114th EPOCH:\n",
      "Training Loss:[0.26484862]|Training Accuracy:1.0|Test Loss:[0.26275392]|Test Accuracy:1.0\n",
      "115th EPOCH:\n",
      "Training Loss:[0.26275392]|Training Accuracy:1.0|Test Loss:[0.26067482]|Test Accuracy:1.0\n",
      "116th EPOCH:\n",
      "Training Loss:[0.26067482]|Training Accuracy:1.0|Test Loss:[0.25861128]|Test Accuracy:1.0\n",
      "117th EPOCH:\n",
      "Training Loss:[0.25861128]|Training Accuracy:1.0|Test Loss:[0.25656328]|Test Accuracy:1.0\n",
      "118th EPOCH:\n",
      "Training Loss:[0.25656328]|Training Accuracy:1.0|Test Loss:[0.25453078]|Test Accuracy:1.0\n",
      "119th EPOCH:\n",
      "Training Loss:[0.25453078]|Training Accuracy:1.0|Test Loss:[0.25251376]|Test Accuracy:1.0\n",
      "120th EPOCH:\n",
      "Training Loss:[0.25251376]|Training Accuracy:1.0|Test Loss:[0.25051217]|Test Accuracy:1.0\n",
      "121th EPOCH:\n",
      "Training Loss:[0.25051217]|Training Accuracy:1.0|Test Loss:[0.248526]|Test Accuracy:1.0\n",
      "122th EPOCH:\n",
      "Training Loss:[0.248526]|Training Accuracy:1.0|Test Loss:[0.24655519]|Test Accuracy:1.0\n",
      "123th EPOCH:\n",
      "Training Loss:[0.24655519]|Training Accuracy:1.0|Test Loss:[0.24459972]|Test Accuracy:1.0\n",
      "124th EPOCH:\n",
      "Training Loss:[0.24459972]|Training Accuracy:1.0|Test Loss:[0.24265955]|Test Accuracy:1.0\n",
      "125th EPOCH:\n",
      "Training Loss:[0.24265955]|Training Accuracy:1.0|Test Loss:[0.24073463]|Test Accuracy:1.0\n",
      "126th EPOCH:\n",
      "Training Loss:[0.24073463]|Training Accuracy:1.0|Test Loss:[0.23882493]|Test Accuracy:1.0\n",
      "127th EPOCH:\n",
      "Training Loss:[0.23882493]|Training Accuracy:1.0|Test Loss:[0.23693041]|Test Accuracy:1.0\n",
      "128th EPOCH:\n",
      "Training Loss:[0.23693041]|Training Accuracy:1.0|Test Loss:[0.23505102]|Test Accuracy:1.0\n",
      "129th EPOCH:\n",
      "Training Loss:[0.23505102]|Training Accuracy:1.0|Test Loss:[0.23318671]|Test Accuracy:1.0\n",
      "130th EPOCH:\n",
      "Training Loss:[0.23318671]|Training Accuracy:1.0|Test Loss:[0.23133745]|Test Accuracy:1.0\n",
      "131th EPOCH:\n",
      "Training Loss:[0.23133745]|Training Accuracy:1.0|Test Loss:[0.22950318]|Test Accuracy:1.0\n",
      "132th EPOCH:\n",
      "Training Loss:[0.22950318]|Training Accuracy:1.0|Test Loss:[0.22768386]|Test Accuracy:1.0\n",
      "133th EPOCH:\n",
      "Training Loss:[0.22768386]|Training Accuracy:1.0|Test Loss:[0.22587943]|Test Accuracy:1.0\n",
      "134th EPOCH:\n",
      "Training Loss:[0.22587943]|Training Accuracy:1.0|Test Loss:[0.22408986]|Test Accuracy:1.0\n",
      "135th EPOCH:\n",
      "Training Loss:[0.22408986]|Training Accuracy:1.0|Test Loss:[0.22231507]|Test Accuracy:1.0\n",
      "136th EPOCH:\n",
      "Training Loss:[0.22231507]|Training Accuracy:1.0|Test Loss:[0.22055503]|Test Accuracy:1.0\n",
      "137th EPOCH:\n",
      "Training Loss:[0.22055503]|Training Accuracy:1.0|Test Loss:[0.21880967]|Test Accuracy:1.0\n",
      "138th EPOCH:\n",
      "Training Loss:[0.21880967]|Training Accuracy:1.0|Test Loss:[0.21707895]|Test Accuracy:1.0\n",
      "139th EPOCH:\n",
      "Training Loss:[0.21707895]|Training Accuracy:1.0|Test Loss:[0.2153628]|Test Accuracy:1.0\n",
      "140th EPOCH:\n",
      "Training Loss:[0.2153628]|Training Accuracy:1.0|Test Loss:[0.21366116]|Test Accuracy:1.0\n",
      "141th EPOCH:\n",
      "Training Loss:[0.21366116]|Training Accuracy:1.0|Test Loss:[0.21197398]|Test Accuracy:1.0\n",
      "142th EPOCH:\n",
      "Training Loss:[0.21197398]|Training Accuracy:1.0|Test Loss:[0.2103012]|Test Accuracy:1.0\n",
      "143th EPOCH:\n",
      "Training Loss:[0.2103012]|Training Accuracy:1.0|Test Loss:[0.20864274]|Test Accuracy:1.0\n",
      "144th EPOCH:\n",
      "Training Loss:[0.20864274]|Training Accuracy:1.0|Test Loss:[0.20699856]|Test Accuracy:1.0\n",
      "145th EPOCH:\n",
      "Training Loss:[0.20699856]|Training Accuracy:1.0|Test Loss:[0.20536858]|Test Accuracy:1.0\n",
      "146th EPOCH:\n",
      "Training Loss:[0.20536858]|Training Accuracy:1.0|Test Loss:[0.20375274]|Test Accuracy:1.0\n",
      "147th EPOCH:\n",
      "Training Loss:[0.20375274]|Training Accuracy:1.0|Test Loss:[0.20215098]|Test Accuracy:1.0\n",
      "148th EPOCH:\n",
      "Training Loss:[0.20215098]|Training Accuracy:1.0|Test Loss:[0.20056321]|Test Accuracy:1.0\n",
      "149th EPOCH:\n",
      "Training Loss:[0.20056321]|Training Accuracy:1.0|Test Loss:[0.19898939]|Test Accuracy:1.0\n",
      "150th EPOCH:\n",
      "Training Loss:[0.19898939]|Training Accuracy:1.0|Test Loss:[0.19742942]|Test Accuracy:1.0\n",
      "151th EPOCH:\n",
      "Training Loss:[0.19742942]|Training Accuracy:1.0|Test Loss:[0.19588325]|Test Accuracy:1.0\n",
      "152th EPOCH:\n",
      "Training Loss:[0.19588325]|Training Accuracy:1.0|Test Loss:[0.1943508]|Test Accuracy:1.0\n",
      "153th EPOCH:\n",
      "Training Loss:[0.1943508]|Training Accuracy:1.0|Test Loss:[0.192832]|Test Accuracy:1.0\n",
      "154th EPOCH:\n",
      "Training Loss:[0.192832]|Training Accuracy:1.0|Test Loss:[0.19132677]|Test Accuracy:1.0\n",
      "155th EPOCH:\n",
      "Training Loss:[0.19132677]|Training Accuracy:1.0|Test Loss:[0.18983503]|Test Accuracy:1.0\n",
      "156th EPOCH:\n",
      "Training Loss:[0.18983503]|Training Accuracy:1.0|Test Loss:[0.18835672]|Test Accuracy:1.0\n",
      "157th EPOCH:\n",
      "Training Loss:[0.18835672]|Training Accuracy:1.0|Test Loss:[0.18689175]|Test Accuracy:1.0\n",
      "158th EPOCH:\n",
      "Training Loss:[0.18689175]|Training Accuracy:1.0|Test Loss:[0.18544004]|Test Accuracy:1.0\n",
      "159th EPOCH:\n",
      "Training Loss:[0.18544004]|Training Accuracy:1.0|Test Loss:[0.18400151]|Test Accuracy:1.0\n",
      "160th EPOCH:\n",
      "Training Loss:[0.18400151]|Training Accuracy:1.0|Test Loss:[0.18257609]|Test Accuracy:1.0\n",
      "161th EPOCH:\n",
      "Training Loss:[0.18257609]|Training Accuracy:1.0|Test Loss:[0.18116369]|Test Accuracy:1.0\n",
      "162th EPOCH:\n",
      "Training Loss:[0.18116369]|Training Accuracy:1.0|Test Loss:[0.17976423]|Test Accuracy:1.0\n",
      "163th EPOCH:\n",
      "Training Loss:[0.17976423]|Training Accuracy:1.0|Test Loss:[0.17837763]|Test Accuracy:1.0\n",
      "164th EPOCH:\n",
      "Training Loss:[0.17837763]|Training Accuracy:1.0|Test Loss:[0.1770038]|Test Accuracy:1.0\n",
      "165th EPOCH:\n",
      "Training Loss:[0.1770038]|Training Accuracy:1.0|Test Loss:[0.17564266]|Test Accuracy:1.0\n",
      "166th EPOCH:\n",
      "Training Loss:[0.17564266]|Training Accuracy:1.0|Test Loss:[0.17429412]|Test Accuracy:1.0\n",
      "167th EPOCH:\n",
      "Training Loss:[0.17429412]|Training Accuracy:1.0|Test Loss:[0.17295811]|Test Accuracy:1.0\n",
      "168th EPOCH:\n",
      "Training Loss:[0.17295811]|Training Accuracy:1.0|Test Loss:[0.17163452]|Test Accuracy:1.0\n",
      "169th EPOCH:\n",
      "Training Loss:[0.17163452]|Training Accuracy:1.0|Test Loss:[0.17032328]|Test Accuracy:1.0\n",
      "170th EPOCH:\n",
      "Training Loss:[0.17032328]|Training Accuracy:1.0|Test Loss:[0.16902429]|Test Accuracy:1.0\n",
      "171th EPOCH:\n",
      "Training Loss:[0.16902429]|Training Accuracy:1.0|Test Loss:[0.16773747]|Test Accuracy:1.0\n",
      "172th EPOCH:\n",
      "Training Loss:[0.16773747]|Training Accuracy:1.0|Test Loss:[0.16646274]|Test Accuracy:1.0\n",
      "173th EPOCH:\n",
      "Training Loss:[0.16646274]|Training Accuracy:1.0|Test Loss:[0.16519999]|Test Accuracy:1.0\n",
      "174th EPOCH:\n",
      "Training Loss:[0.16519999]|Training Accuracy:1.0|Test Loss:[0.16394915]|Test Accuracy:1.0\n",
      "175th EPOCH:\n",
      "Training Loss:[0.16394915]|Training Accuracy:1.0|Test Loss:[0.16271012]|Test Accuracy:1.0\n",
      "176th EPOCH:\n",
      "Training Loss:[0.16271012]|Training Accuracy:1.0|Test Loss:[0.1614828]|Test Accuracy:1.0\n",
      "177th EPOCH:\n",
      "Training Loss:[0.1614828]|Training Accuracy:1.0|Test Loss:[0.16026712]|Test Accuracy:1.0\n",
      "178th EPOCH:\n",
      "Training Loss:[0.16026712]|Training Accuracy:1.0|Test Loss:[0.15906298]|Test Accuracy:1.0\n",
      "179th EPOCH:\n",
      "Training Loss:[0.15906298]|Training Accuracy:1.0|Test Loss:[0.15787029]|Test Accuracy:1.0\n",
      "180th EPOCH:\n",
      "Training Loss:[0.15787029]|Training Accuracy:1.0|Test Loss:[0.15668896]|Test Accuracy:1.0\n",
      "181th EPOCH:\n",
      "Training Loss:[0.15668896]|Training Accuracy:1.0|Test Loss:[0.15551889]|Test Accuracy:1.0\n",
      "182th EPOCH:\n",
      "Training Loss:[0.15551889]|Training Accuracy:1.0|Test Loss:[0.15435999]|Test Accuracy:1.0\n",
      "183th EPOCH:\n",
      "Training Loss:[0.15435999]|Training Accuracy:1.0|Test Loss:[0.15321218]|Test Accuracy:1.0\n",
      "184th EPOCH:\n",
      "Training Loss:[0.15321218]|Training Accuracy:1.0|Test Loss:[0.15207535]|Test Accuracy:1.0\n",
      "185th EPOCH:\n",
      "Training Loss:[0.15207535]|Training Accuracy:1.0|Test Loss:[0.15094942]|Test Accuracy:1.0\n",
      "186th EPOCH:\n",
      "Training Loss:[0.15094942]|Training Accuracy:1.0|Test Loss:[0.1498343]|Test Accuracy:1.0\n",
      "187th EPOCH:\n",
      "Training Loss:[0.1498343]|Training Accuracy:1.0|Test Loss:[0.14872989]|Test Accuracy:1.0\n",
      "188th EPOCH:\n",
      "Training Loss:[0.14872989]|Training Accuracy:1.0|Test Loss:[0.1476361]|Test Accuracy:1.0\n",
      "189th EPOCH:\n",
      "Training Loss:[0.1476361]|Training Accuracy:1.0|Test Loss:[0.14655284]|Test Accuracy:1.0\n",
      "190th EPOCH:\n",
      "Training Loss:[0.14655284]|Training Accuracy:1.0|Test Loss:[0.14548001]|Test Accuracy:1.0\n",
      "191th EPOCH:\n",
      "Training Loss:[0.14548001]|Training Accuracy:1.0|Test Loss:[0.14441753]|Test Accuracy:1.0\n",
      "192th EPOCH:\n",
      "Training Loss:[0.14441753]|Training Accuracy:1.0|Test Loss:[0.1433653]|Test Accuracy:1.0\n",
      "193th EPOCH:\n",
      "Training Loss:[0.1433653]|Training Accuracy:1.0|Test Loss:[0.14232323]|Test Accuracy:1.0\n",
      "194th EPOCH:\n",
      "Training Loss:[0.14232323]|Training Accuracy:1.0|Test Loss:[0.14129123]|Test Accuracy:1.0\n",
      "195th EPOCH:\n",
      "Training Loss:[0.14129123]|Training Accuracy:1.0|Test Loss:[0.14026921]|Test Accuracy:1.0\n",
      "196th EPOCH:\n",
      "Training Loss:[0.14026921]|Training Accuracy:1.0|Test Loss:[0.13925707]|Test Accuracy:1.0\n",
      "197th EPOCH:\n",
      "Training Loss:[0.13925707]|Training Accuracy:1.0|Test Loss:[0.13825473]|Test Accuracy:1.0\n",
      "198th EPOCH:\n",
      "Training Loss:[0.13825473]|Training Accuracy:1.0|Test Loss:[0.13726209]|Test Accuracy:1.0\n",
      "199th EPOCH:\n",
      "Training Loss:[0.13726209]|Training Accuracy:1.0|Test Loss:[0.13627906]|Test Accuracy:1.0\n"
     ]
    }
   ],
   "source": [
    "# Now we can train the model by iteratively on each datapoint.\n",
    "\n",
    "layer_list = [2,3, 1]\n",
    "activation_list = ['sigmoid','sigmoid']\n",
    "model = MultiLayerPerceptron(layer_list,activation_list)\n",
    "\n",
    "\n",
    "_=model.train(X_train,\n",
    "            Y_train,\n",
    "            X_train,\n",
    "            Y_train,\n",
    "            metric ='accuracy_binary',\n",
    "            loss_function_string='mean_binary_cross_entropy',\n",
    "            epochs=200,\n",
    "            record_at=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now we can predict the values for unseen data or trained data also\n",
    "# We can also calculate the accuracy of the model we have trained\n",
    "model.metric_function(X_train,Y_train,metric='accuracy_binary')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# N Bit XOR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now Lets try working with just a little better data. A n XOR operator. So lets create the dataset for n bit xor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We would have atmost 2^n data point in this type of data set.BUt we would limit our dataset to a 1000 data points\n",
    "whichever is smaller.\n",
    "\n",
    "Then we can divide into training and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "n = 10\n",
    "max_datapoint = 10000\n",
    "datapoints = min(pow(2,n) , max_datapoint)\n",
    "\n",
    "X = np.zeros((datapoints , n) , dtype=np.int32)\n",
    "Y = np.zeros((datapoints , 1), dtype=np.int32)\n",
    "\n",
    "for i in range(datapoints):\n",
    "    tmp = i\n",
    "    y_tmp = 0\n",
    "    for j in range(n-1 , -1 , -1):\n",
    "        X[i,j] = tmp&1\n",
    "        y_tmp = y_tmp^X[i,j]\n",
    "        tmp = tmp>>1\n",
    "    Y[i] = y_tmp\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for sanity check lets print one example\n",
    "ind = 11\n",
    "print(X[ind] , Y[ind])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets divide the set in training and testing\n",
    "div = 0.9\n",
    "train_n = int(div * datapoints)\n",
    "X_train = X[:train_n]\n",
    "Y_train = Y[:train_n]\n",
    "\n",
    "X_test = X[train_n:]\n",
    "Y_test = Y[train_n:]\n",
    "Y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_list = [n,20,20,15,8,4,1]\n",
    "activation_list = ['relu','sigmoid','sigmoid','sigmoid','tanh','sigmoid']\n",
    "\n",
    "model = MultiLayerPerceptron(layer_list,activation_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_list = [n,20,20,15,8,4,1]\n",
    "activation_list = ['relu','sigmoid','sigmoid','sigmoid','tanh','sigmoid']\n",
    "\n",
    "model = MultiLayerPerceptron(layer_list,activation_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_loss_his,train_acc_his,test_loss_his,test_acc_his,epoch_his = model.train(X_train,\n",
    "                                                                                Y_train ,\n",
    "                                                                                X_test,\n",
    "                                                                                Y_test,\n",
    "                                                                                accuracy_fn=accuracy,\n",
    "                                                                                loss_function_string='mean_binary_cross_entropy',\n",
    "                                                                                epochs = 500,\n",
    "                                                                                record_at = 20,\n",
    "                                                                                learning_rate= 0.1,\n",
    "                                                                                learning_rate_decay=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_line, = plt.plot(epoch_his,train_loss_his,label = 'train')\n",
    "test_line, = plt.plot(epoch_his,test_loss_his,label = 'test')\n",
    "plt.xlabel('EPOCHS')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend([train_line, test_line] , ['train','test'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_line, = plt.plot(epoch_his,train_acc_his,label = 'train')\n",
    "test_line, = plt.plot(epoch_his,test_acc_his,label = 'test')\n",
    "plt.xlabel('EPOCHS')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend([train_line, test_line] , ['train','test'])\n",
    "plt.show()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
