{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perceptron\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'divide': 'warn', 'over': 'warn', 'under': 'ignore', 'invalid': 'warn'}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.seterr('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perceptron Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here I am building a normal forward propogation for a perceptron layer\n",
    "# and the various function i would use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################\n",
    "## Declaring Training Data        ############\n",
    "#############################################\n",
    "X_train = np.array([[0,0],[1,0],[0,1],[1,1]])\n",
    "Y_train = np.array([[1],[0],[0],[0]])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from MultiLayerPerceptron import MultiLayerPerceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(model , X , Y):\n",
    "    pred,_ = model.forward(X)\n",
    "    pred =pred > 0.5\n",
    "    acc = np.sum(pred == Y)\n",
    "    acc = float(acc) / Y.shape[0]\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 1) (1,)\n",
      "<function sigmoid at 0x7fe0aa99a378>\n"
     ]
    }
   ],
   "source": [
    "# Declare a neuron with shape of weights as [shape_of_input,1]\n",
    "model = MultiLayerPerceptron([2,1],['sigmoid'])\n",
    "# print(model.layers[1].W)\n",
    "# print(model.layers[1].b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing the model outputs\n",
    "pred , _ = model.forward(X_train)\n",
    "# print(np.sum((pred > 0.5)== Y_train) / Y_train.shape[0])\n",
    "# accuracy(model , X_train,Y_train)\n",
    "# X_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The loss is [0.04716012]\n",
      "And the error to be back propogated is:\n",
      " [[-0.125     ]\n",
      " [ 0.06907923]\n",
      " [ 0.05168377]\n",
      " [ 0.02262543]]\n"
     ]
    }
   ],
   "source": [
    "# Checking for testing purposes(BCE should be used here ideally)\n",
    "from Loss import mean_abs_error,mean_square_error\n",
    "loss,d_back = mean_square_error(pred,Y_train)\n",
    "print(\"The loss is {}\\nAnd the error to be back propogated is:\\n {}\".format(loss , d_back))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The loss is [1.34300907]\n",
      "And the error to be back propogated is:\n",
      " [[-2.        ]\n",
      " [ 1.38182031]\n",
      " [ 1.2606129 ]\n",
      " [ 1.0995073 ]]\n"
     ]
    }
   ],
   "source": [
    "from Loss import binary_cross_entropy,mean_binary_cross_entropy\n",
    "loss,d_back = binary_cross_entropy(pred,Y_train)\n",
    "print(\"The loss is {}\\nAnd the error to be back propogated is:\\n {}\".format(loss , d_back))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 3) (3,)\n",
      "<function sigmoid at 0x7fe0aa99a378>\n",
      "(3, 1) (1,)\n",
      "<function sigmoid at 0x7fe0aa99a378>\n",
      "The training loss at 0th epoch : [0.83240777]  Training Accuracy:0.25\n",
      "The training loss at 1th epoch : [0.79414997]  Training Accuracy:0.25\n",
      "The training loss at 2th epoch : [0.7622246]  Training Accuracy:0.5\n",
      "The training loss at 3th epoch : [0.73544571]  Training Accuracy:0.5\n",
      "The training loss at 4th epoch : [0.71284675]  Training Accuracy:0.5\n",
      "The training loss at 5th epoch : [0.69364287]  Training Accuracy:0.75\n",
      "The training loss at 6th epoch : [0.67719796]  Training Accuracy:0.75\n",
      "The training loss at 7th epoch : [0.66299648]  Training Accuracy:0.5\n",
      "The training loss at 8th epoch : [0.65062003]  Training Accuracy:0.5\n",
      "The training loss at 9th epoch : [0.63972822]  Training Accuracy:0.5\n",
      "The training loss at 10th epoch : [0.63004314]  Training Accuracy:0.5\n",
      "The training loss at 11th epoch : [0.6213369]  Training Accuracy:0.5\n",
      "The training loss at 12th epoch : [0.61342181]  Training Accuracy:0.5\n",
      "The training loss at 13th epoch : [0.60614238]  Training Accuracy:0.5\n",
      "The training loss at 14th epoch : [0.59936918]  Training Accuracy:0.5\n",
      "The training loss at 15th epoch : [0.59299375]  Training Accuracy:0.75\n",
      "The training loss at 16th epoch : [0.58692472]  Training Accuracy:0.75\n",
      "The training loss at 17th epoch : [0.58108464]  Training Accuracy:0.75\n",
      "The training loss at 18th epoch : [0.5754074]  Training Accuracy:0.75\n",
      "The training loss at 19th epoch : [0.56983624]  Training Accuracy:0.75\n"
     ]
    }
   ],
   "source": [
    "# Now we can train the model by iteratively on each datapoint.\n",
    "\n",
    "layer_list = [2,3, 1]\n",
    "activation_list = ['sigmoid','sigmoid']\n",
    "model = MultiLayerPerceptron(layer_list,activation_list)\n",
    "\n",
    "def train(model ,X_train , Y_train,epochs,alpha =0.1):\n",
    "    for i in range(epochs):\n",
    "        act , cache = model.forward(X_train)\n",
    "        loss,d_back= mean_binary_cross_entropy(act,Y_train)\n",
    "        acc = accuracy(model ,X_train,Y_train)\n",
    "        print(\"The training loss at {}th epoch : {}  Training Accuracy:{}\".format(i , loss , acc))\n",
    "        model.update_gradient(cache,d_back,alpha)        \n",
    "\n",
    "train(model , X_train,Y_train , 20,alpha = 0.4)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.75"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now we can predict the values for unseen data or trained data also\n",
    "# We can also calculate the accuracy of the model we have trained\n",
    "accuracy(model , X_train,Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# N Bit XOR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now Lets try working with just a little better data. A n XOR operator. So lets create the dataset for n bit xor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We would have atmost 2^n data point in this type of data set.BUt we would limit our dataset to a 1000 data points\n",
    "whichever is smaller.\n",
    "\n",
    "Then we can divide into training and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "n = 10\n",
    "max_datapoint = 10000\n",
    "datapoints = min(pow(2,n) , max_datapoint)\n",
    "\n",
    "X = np.zeros((datapoints , n) , dtype=np.int32)\n",
    "Y = np.zeros((datapoints , 1), dtype=np.int32)\n",
    "\n",
    "for i in range(datapoints):\n",
    "    tmp = i\n",
    "    y_tmp = 0\n",
    "    for j in range(n-1 , -1 , -1):\n",
    "        X[i,j] = tmp&1\n",
    "        y_tmp = y_tmp^X[i,j]\n",
    "        tmp = tmp>>1\n",
    "    Y[i] = y_tmp\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 1 0 1 1] [1]\n"
     ]
    }
   ],
   "source": [
    "# for sanity check lets print one example\n",
    "ind = 11\n",
    "print(X[ind] , Y[ind])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(103, 1)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lets divide the set in training and testing\n",
    "div = 0.9\n",
    "train_n = int(div * datapoints)\n",
    "X_train = X[:train_n]\n",
    "Y_train = Y[:train_n]\n",
    "\n",
    "X_test = X[train_n:]\n",
    "Y_test = Y[train_n:]\n",
    "Y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 20) (20,)\n",
      "<function relu at 0x7fe0aa99a620>\n",
      "(20, 20) (20,)\n",
      "<function sigmoid at 0x7fe0aa99a378>\n",
      "(20, 15) (15,)\n",
      "<function sigmoid at 0x7fe0aa99a378>\n",
      "(15, 8) (8,)\n",
      "<function sigmoid at 0x7fe0aa99a378>\n",
      "(8, 4) (4,)\n",
      "<function tanh at 0x7fe0aa99a510>\n",
      "(4, 1) (1,)\n",
      "<function sigmoid at 0x7fe0aa99a378>\n"
     ]
    }
   ],
   "source": [
    "layer_list = [n,20,20,15,8,4,1]\n",
    "activation_list = ['relu','sigmoid','sigmoid','sigmoid','tanh','sigmoid']\n",
    "\n",
    "model = MultiLayerPerceptron(layer_list,activation_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training loss at 0th epoch : [0.83916784]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1th epoch : [0.83887533]  Training Accuracy:0.500542888165038\n",
      "The training loss at 2th epoch : [0.83858336]  Training Accuracy:0.500542888165038\n",
      "The training loss at 3th epoch : [0.83829192]  Training Accuracy:0.500542888165038\n",
      "The training loss at 4th epoch : [0.83800101]  Training Accuracy:0.500542888165038\n",
      "The training loss at 5th epoch : [0.83771063]  Training Accuracy:0.500542888165038\n",
      "The training loss at 6th epoch : [0.83742078]  Training Accuracy:0.500542888165038\n",
      "The training loss at 7th epoch : [0.83713145]  Training Accuracy:0.500542888165038\n",
      "The training loss at 8th epoch : [0.83684265]  Training Accuracy:0.500542888165038\n",
      "The training loss at 9th epoch : [0.83655438]  Training Accuracy:0.500542888165038\n",
      "The training loss at 10th epoch : [0.83626663]  Training Accuracy:0.500542888165038\n",
      "The training loss at 11th epoch : [0.83597941]  Training Accuracy:0.500542888165038\n",
      "The training loss at 12th epoch : [0.83569271]  Training Accuracy:0.500542888165038\n",
      "The training loss at 13th epoch : [0.83540654]  Training Accuracy:0.500542888165038\n",
      "The training loss at 14th epoch : [0.83512088]  Training Accuracy:0.500542888165038\n",
      "The training loss at 15th epoch : [0.83483575]  Training Accuracy:0.500542888165038\n",
      "The training loss at 16th epoch : [0.83455114]  Training Accuracy:0.500542888165038\n",
      "The training loss at 17th epoch : [0.83426705]  Training Accuracy:0.500542888165038\n",
      "The training loss at 18th epoch : [0.83398348]  Training Accuracy:0.500542888165038\n",
      "The training loss at 19th epoch : [0.83370043]  Training Accuracy:0.500542888165038\n",
      "The training loss at 20th epoch : [0.8334179]  Training Accuracy:0.500542888165038\n",
      "The training loss at 21th epoch : [0.83313588]  Training Accuracy:0.500542888165038\n",
      "The training loss at 22th epoch : [0.83285438]  Training Accuracy:0.500542888165038\n",
      "The training loss at 23th epoch : [0.83257339]  Training Accuracy:0.500542888165038\n",
      "The training loss at 24th epoch : [0.83229292]  Training Accuracy:0.500542888165038\n",
      "The training loss at 25th epoch : [0.83201297]  Training Accuracy:0.500542888165038\n",
      "The training loss at 26th epoch : [0.83173352]  Training Accuracy:0.500542888165038\n",
      "The training loss at 27th epoch : [0.83145459]  Training Accuracy:0.500542888165038\n",
      "The training loss at 28th epoch : [0.83117617]  Training Accuracy:0.500542888165038\n",
      "The training loss at 29th epoch : [0.83089827]  Training Accuracy:0.500542888165038\n",
      "The training loss at 30th epoch : [0.83062087]  Training Accuracy:0.500542888165038\n",
      "The training loss at 31th epoch : [0.83034398]  Training Accuracy:0.500542888165038\n",
      "The training loss at 32th epoch : [0.83006761]  Training Accuracy:0.500542888165038\n",
      "The training loss at 33th epoch : [0.82979174]  Training Accuracy:0.500542888165038\n",
      "The training loss at 34th epoch : [0.82951637]  Training Accuracy:0.500542888165038\n",
      "The training loss at 35th epoch : [0.82924152]  Training Accuracy:0.500542888165038\n",
      "The training loss at 36th epoch : [0.82896717]  Training Accuracy:0.500542888165038\n",
      "The training loss at 37th epoch : [0.82869332]  Training Accuracy:0.500542888165038\n",
      "The training loss at 38th epoch : [0.82841998]  Training Accuracy:0.500542888165038\n",
      "The training loss at 39th epoch : [0.82814714]  Training Accuracy:0.500542888165038\n",
      "The training loss at 40th epoch : [0.82787481]  Training Accuracy:0.500542888165038\n",
      "The training loss at 41th epoch : [0.82760298]  Training Accuracy:0.500542888165038\n",
      "The training loss at 42th epoch : [0.82733165]  Training Accuracy:0.500542888165038\n",
      "The training loss at 43th epoch : [0.82706082]  Training Accuracy:0.500542888165038\n",
      "The training loss at 44th epoch : [0.82679049]  Training Accuracy:0.500542888165038\n",
      "The training loss at 45th epoch : [0.82652066]  Training Accuracy:0.500542888165038\n",
      "The training loss at 46th epoch : [0.82625132]  Training Accuracy:0.500542888165038\n",
      "The training loss at 47th epoch : [0.82598249]  Training Accuracy:0.500542888165038\n",
      "The training loss at 48th epoch : [0.82571415]  Training Accuracy:0.500542888165038\n",
      "The training loss at 49th epoch : [0.82544631]  Training Accuracy:0.500542888165038\n",
      "The training loss at 50th epoch : [0.82517896]  Training Accuracy:0.500542888165038\n",
      "The training loss at 51th epoch : [0.82491211]  Training Accuracy:0.500542888165038\n",
      "The training loss at 52th epoch : [0.82464576]  Training Accuracy:0.500542888165038\n",
      "The training loss at 53th epoch : [0.82437989]  Training Accuracy:0.500542888165038\n",
      "The training loss at 54th epoch : [0.82411452]  Training Accuracy:0.500542888165038\n",
      "The training loss at 55th epoch : [0.82384964]  Training Accuracy:0.500542888165038\n",
      "The training loss at 56th epoch : [0.82358525]  Training Accuracy:0.500542888165038\n",
      "The training loss at 57th epoch : [0.82332135]  Training Accuracy:0.500542888165038\n",
      "The training loss at 58th epoch : [0.82305794]  Training Accuracy:0.500542888165038\n",
      "The training loss at 59th epoch : [0.82279502]  Training Accuracy:0.500542888165038\n",
      "The training loss at 60th epoch : [0.82253259]  Training Accuracy:0.500542888165038\n",
      "The training loss at 61th epoch : [0.82227065]  Training Accuracy:0.500542888165038\n",
      "The training loss at 62th epoch : [0.82200919]  Training Accuracy:0.500542888165038\n",
      "The training loss at 63th epoch : [0.82174822]  Training Accuracy:0.500542888165038\n",
      "The training loss at 64th epoch : [0.82148773]  Training Accuracy:0.500542888165038\n",
      "The training loss at 65th epoch : [0.82122773]  Training Accuracy:0.500542888165038\n",
      "The training loss at 66th epoch : [0.82096821]  Training Accuracy:0.500542888165038\n",
      "The training loss at 67th epoch : [0.82070918]  Training Accuracy:0.500542888165038\n",
      "The training loss at 68th epoch : [0.82045062]  Training Accuracy:0.500542888165038\n",
      "The training loss at 69th epoch : [0.82019255]  Training Accuracy:0.500542888165038\n",
      "The training loss at 70th epoch : [0.81993496]  Training Accuracy:0.500542888165038\n",
      "The training loss at 71th epoch : [0.81967785]  Training Accuracy:0.500542888165038\n",
      "The training loss at 72th epoch : [0.81942122]  Training Accuracy:0.500542888165038\n",
      "The training loss at 73th epoch : [0.81916506]  Training Accuracy:0.500542888165038\n",
      "The training loss at 74th epoch : [0.81890939]  Training Accuracy:0.500542888165038\n",
      "The training loss at 75th epoch : [0.81865419]  Training Accuracy:0.500542888165038\n",
      "The training loss at 76th epoch : [0.81839947]  Training Accuracy:0.500542888165038\n",
      "The training loss at 77th epoch : [0.81814522]  Training Accuracy:0.500542888165038\n",
      "The training loss at 78th epoch : [0.81789145]  Training Accuracy:0.500542888165038\n",
      "The training loss at 79th epoch : [0.81763815]  Training Accuracy:0.500542888165038\n",
      "The training loss at 80th epoch : [0.81738533]  Training Accuracy:0.500542888165038\n",
      "The training loss at 81th epoch : [0.81713298]  Training Accuracy:0.500542888165038\n",
      "The training loss at 82th epoch : [0.8168811]  Training Accuracy:0.500542888165038\n",
      "The training loss at 83th epoch : [0.81662969]  Training Accuracy:0.500542888165038\n",
      "The training loss at 84th epoch : [0.81637875]  Training Accuracy:0.500542888165038\n",
      "The training loss at 85th epoch : [0.81612829]  Training Accuracy:0.500542888165038\n",
      "The training loss at 86th epoch : [0.81587829]  Training Accuracy:0.500542888165038\n",
      "The training loss at 87th epoch : [0.81562876]  Training Accuracy:0.500542888165038\n",
      "The training loss at 88th epoch : [0.8153797]  Training Accuracy:0.500542888165038\n",
      "The training loss at 89th epoch : [0.8151311]  Training Accuracy:0.500542888165038\n",
      "The training loss at 90th epoch : [0.81488297]  Training Accuracy:0.500542888165038\n",
      "The training loss at 91th epoch : [0.81463531]  Training Accuracy:0.500542888165038\n",
      "The training loss at 92th epoch : [0.81438811]  Training Accuracy:0.500542888165038\n",
      "The training loss at 93th epoch : [0.81414138]  Training Accuracy:0.500542888165038\n",
      "The training loss at 94th epoch : [0.81389511]  Training Accuracy:0.500542888165038\n",
      "The training loss at 95th epoch : [0.8136493]  Training Accuracy:0.500542888165038\n",
      "The training loss at 96th epoch : [0.81340395]  Training Accuracy:0.500542888165038\n",
      "The training loss at 97th epoch : [0.81315907]  Training Accuracy:0.500542888165038\n",
      "The training loss at 98th epoch : [0.81291464]  Training Accuracy:0.500542888165038\n",
      "The training loss at 99th epoch : [0.81267068]  Training Accuracy:0.500542888165038\n",
      "The training loss at 100th epoch : [0.81242717]  Training Accuracy:0.500542888165038\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training loss at 101th epoch : [0.81218413]  Training Accuracy:0.500542888165038\n",
      "The training loss at 102th epoch : [0.81194154]  Training Accuracy:0.500542888165038\n",
      "The training loss at 103th epoch : [0.81169941]  Training Accuracy:0.500542888165038\n",
      "The training loss at 104th epoch : [0.81145773]  Training Accuracy:0.500542888165038\n",
      "The training loss at 105th epoch : [0.81121651]  Training Accuracy:0.500542888165038\n",
      "The training loss at 106th epoch : [0.81097574]  Training Accuracy:0.500542888165038\n",
      "The training loss at 107th epoch : [0.81073543]  Training Accuracy:0.500542888165038\n",
      "The training loss at 108th epoch : [0.81049557]  Training Accuracy:0.500542888165038\n",
      "The training loss at 109th epoch : [0.81025616]  Training Accuracy:0.500542888165038\n",
      "The training loss at 110th epoch : [0.81001721]  Training Accuracy:0.500542888165038\n",
      "The training loss at 111th epoch : [0.80977871]  Training Accuracy:0.500542888165038\n",
      "The training loss at 112th epoch : [0.80954066]  Training Accuracy:0.500542888165038\n",
      "The training loss at 113th epoch : [0.80930305]  Training Accuracy:0.500542888165038\n",
      "The training loss at 114th epoch : [0.8090659]  Training Accuracy:0.500542888165038\n",
      "The training loss at 115th epoch : [0.80882919]  Training Accuracy:0.500542888165038\n",
      "The training loss at 116th epoch : [0.80859294]  Training Accuracy:0.500542888165038\n",
      "The training loss at 117th epoch : [0.80835713]  Training Accuracy:0.500542888165038\n",
      "The training loss at 118th epoch : [0.80812176]  Training Accuracy:0.500542888165038\n",
      "The training loss at 119th epoch : [0.80788685]  Training Accuracy:0.500542888165038\n",
      "The training loss at 120th epoch : [0.80765237]  Training Accuracy:0.500542888165038\n",
      "The training loss at 121th epoch : [0.80741834]  Training Accuracy:0.500542888165038\n",
      "The training loss at 122th epoch : [0.80718476]  Training Accuracy:0.500542888165038\n",
      "The training loss at 123th epoch : [0.80695161]  Training Accuracy:0.500542888165038\n",
      "The training loss at 124th epoch : [0.80671891]  Training Accuracy:0.500542888165038\n",
      "The training loss at 125th epoch : [0.80648665]  Training Accuracy:0.500542888165038\n",
      "The training loss at 126th epoch : [0.80625483]  Training Accuracy:0.500542888165038\n",
      "The training loss at 127th epoch : [0.80602346]  Training Accuracy:0.500542888165038\n",
      "The training loss at 128th epoch : [0.80579252]  Training Accuracy:0.500542888165038\n",
      "The training loss at 129th epoch : [0.80556201]  Training Accuracy:0.500542888165038\n",
      "The training loss at 130th epoch : [0.80533195]  Training Accuracy:0.500542888165038\n",
      "The training loss at 131th epoch : [0.80510233]  Training Accuracy:0.500542888165038\n",
      "The training loss at 132th epoch : [0.80487314]  Training Accuracy:0.500542888165038\n",
      "The training loss at 133th epoch : [0.80464438]  Training Accuracy:0.500542888165038\n",
      "The training loss at 134th epoch : [0.80441606]  Training Accuracy:0.500542888165038\n",
      "The training loss at 135th epoch : [0.80418818]  Training Accuracy:0.500542888165038\n",
      "The training loss at 136th epoch : [0.80396073]  Training Accuracy:0.500542888165038\n",
      "The training loss at 137th epoch : [0.80373371]  Training Accuracy:0.500542888165038\n",
      "The training loss at 138th epoch : [0.80350713]  Training Accuracy:0.500542888165038\n",
      "The training loss at 139th epoch : [0.80328097]  Training Accuracy:0.500542888165038\n",
      "The training loss at 140th epoch : [0.80305525]  Training Accuracy:0.500542888165038\n",
      "The training loss at 141th epoch : [0.80282996]  Training Accuracy:0.500542888165038\n",
      "The training loss at 142th epoch : [0.80260509]  Training Accuracy:0.500542888165038\n",
      "The training loss at 143th epoch : [0.80238066]  Training Accuracy:0.500542888165038\n",
      "The training loss at 144th epoch : [0.80215665]  Training Accuracy:0.500542888165038\n",
      "The training loss at 145th epoch : [0.80193308]  Training Accuracy:0.500542888165038\n",
      "The training loss at 146th epoch : [0.80170992]  Training Accuracy:0.500542888165038\n",
      "The training loss at 147th epoch : [0.8014872]  Training Accuracy:0.500542888165038\n",
      "The training loss at 148th epoch : [0.8012649]  Training Accuracy:0.500542888165038\n",
      "The training loss at 149th epoch : [0.80104302]  Training Accuracy:0.500542888165038\n",
      "The training loss at 150th epoch : [0.80082157]  Training Accuracy:0.500542888165038\n",
      "The training loss at 151th epoch : [0.80060054]  Training Accuracy:0.500542888165038\n",
      "The training loss at 152th epoch : [0.80037994]  Training Accuracy:0.500542888165038\n",
      "The training loss at 153th epoch : [0.80015976]  Training Accuracy:0.500542888165038\n",
      "The training loss at 154th epoch : [0.79993999]  Training Accuracy:0.500542888165038\n",
      "The training loss at 155th epoch : [0.79972065]  Training Accuracy:0.500542888165038\n",
      "The training loss at 156th epoch : [0.79950173]  Training Accuracy:0.500542888165038\n",
      "The training loss at 157th epoch : [0.79928323]  Training Accuracy:0.500542888165038\n",
      "The training loss at 158th epoch : [0.79906515]  Training Accuracy:0.500542888165038\n",
      "The training loss at 159th epoch : [0.79884748]  Training Accuracy:0.500542888165038\n",
      "The training loss at 160th epoch : [0.79863023]  Training Accuracy:0.500542888165038\n",
      "The training loss at 161th epoch : [0.7984134]  Training Accuracy:0.500542888165038\n",
      "The training loss at 162th epoch : [0.79819698]  Training Accuracy:0.500542888165038\n",
      "The training loss at 163th epoch : [0.79798098]  Training Accuracy:0.500542888165038\n",
      "The training loss at 164th epoch : [0.7977654]  Training Accuracy:0.500542888165038\n",
      "The training loss at 165th epoch : [0.79755022]  Training Accuracy:0.500542888165038\n",
      "The training loss at 166th epoch : [0.79733546]  Training Accuracy:0.500542888165038\n",
      "The training loss at 167th epoch : [0.79712112]  Training Accuracy:0.500542888165038\n",
      "The training loss at 168th epoch : [0.79690718]  Training Accuracy:0.500542888165038\n",
      "The training loss at 169th epoch : [0.79669366]  Training Accuracy:0.500542888165038\n",
      "The training loss at 170th epoch : [0.79648054]  Training Accuracy:0.500542888165038\n",
      "The training loss at 171th epoch : [0.79626784]  Training Accuracy:0.500542888165038\n",
      "The training loss at 172th epoch : [0.79605555]  Training Accuracy:0.500542888165038\n",
      "The training loss at 173th epoch : [0.79584366]  Training Accuracy:0.500542888165038\n",
      "The training loss at 174th epoch : [0.79563218]  Training Accuracy:0.500542888165038\n",
      "The training loss at 175th epoch : [0.79542111]  Training Accuracy:0.500542888165038\n",
      "The training loss at 176th epoch : [0.79521045]  Training Accuracy:0.500542888165038\n",
      "The training loss at 177th epoch : [0.79500019]  Training Accuracy:0.500542888165038\n",
      "The training loss at 178th epoch : [0.79479033]  Training Accuracy:0.500542888165038\n",
      "The training loss at 179th epoch : [0.79458088]  Training Accuracy:0.500542888165038\n",
      "The training loss at 180th epoch : [0.79437184]  Training Accuracy:0.500542888165038\n",
      "The training loss at 181th epoch : [0.79416319]  Training Accuracy:0.500542888165038\n",
      "The training loss at 182th epoch : [0.79395495]  Training Accuracy:0.500542888165038\n",
      "The training loss at 183th epoch : [0.79374712]  Training Accuracy:0.500542888165038\n",
      "The training loss at 184th epoch : [0.79353968]  Training Accuracy:0.500542888165038\n",
      "The training loss at 185th epoch : [0.79333264]  Training Accuracy:0.500542888165038\n",
      "The training loss at 186th epoch : [0.793126]  Training Accuracy:0.500542888165038\n",
      "The training loss at 187th epoch : [0.79291977]  Training Accuracy:0.500542888165038\n",
      "The training loss at 188th epoch : [0.79271393]  Training Accuracy:0.500542888165038\n",
      "The training loss at 189th epoch : [0.79250848]  Training Accuracy:0.500542888165038\n",
      "The training loss at 190th epoch : [0.79230344]  Training Accuracy:0.500542888165038\n",
      "The training loss at 191th epoch : [0.79209879]  Training Accuracy:0.500542888165038\n",
      "The training loss at 192th epoch : [0.79189454]  Training Accuracy:0.500542888165038\n",
      "The training loss at 193th epoch : [0.79169068]  Training Accuracy:0.500542888165038\n",
      "The training loss at 194th epoch : [0.79148722]  Training Accuracy:0.500542888165038\n",
      "The training loss at 195th epoch : [0.79128415]  Training Accuracy:0.500542888165038\n",
      "The training loss at 196th epoch : [0.79108147]  Training Accuracy:0.500542888165038\n",
      "The training loss at 197th epoch : [0.79087919]  Training Accuracy:0.500542888165038\n",
      "The training loss at 198th epoch : [0.7906773]  Training Accuracy:0.500542888165038\n",
      "The training loss at 199th epoch : [0.7904758]  Training Accuracy:0.500542888165038\n",
      "The training loss at 200th epoch : [0.79027469]  Training Accuracy:0.500542888165038\n",
      "The training loss at 201th epoch : [0.79007397]  Training Accuracy:0.500542888165038\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training loss at 202th epoch : [0.78987364]  Training Accuracy:0.500542888165038\n",
      "The training loss at 203th epoch : [0.7896737]  Training Accuracy:0.500542888165038\n",
      "The training loss at 204th epoch : [0.78947414]  Training Accuracy:0.500542888165038\n",
      "The training loss at 205th epoch : [0.78927498]  Training Accuracy:0.500542888165038\n",
      "The training loss at 206th epoch : [0.7890762]  Training Accuracy:0.500542888165038\n",
      "The training loss at 207th epoch : [0.7888778]  Training Accuracy:0.500542888165038\n",
      "The training loss at 208th epoch : [0.7886798]  Training Accuracy:0.500542888165038\n",
      "The training loss at 209th epoch : [0.78848217]  Training Accuracy:0.500542888165038\n",
      "The training loss at 210th epoch : [0.78828493]  Training Accuracy:0.500542888165038\n",
      "The training loss at 211th epoch : [0.78808808]  Training Accuracy:0.500542888165038\n",
      "The training loss at 212th epoch : [0.7878916]  Training Accuracy:0.500542888165038\n",
      "The training loss at 213th epoch : [0.78769551]  Training Accuracy:0.500542888165038\n",
      "The training loss at 214th epoch : [0.7874998]  Training Accuracy:0.500542888165038\n",
      "The training loss at 215th epoch : [0.78730447]  Training Accuracy:0.500542888165038\n",
      "The training loss at 216th epoch : [0.78710952]  Training Accuracy:0.500542888165038\n",
      "The training loss at 217th epoch : [0.78691496]  Training Accuracy:0.500542888165038\n",
      "The training loss at 218th epoch : [0.78672077]  Training Accuracy:0.500542888165038\n",
      "The training loss at 219th epoch : [0.78652695]  Training Accuracy:0.500542888165038\n",
      "The training loss at 220th epoch : [0.78633352]  Training Accuracy:0.500542888165038\n",
      "The training loss at 221th epoch : [0.78614046]  Training Accuracy:0.500542888165038\n",
      "The training loss at 222th epoch : [0.78594778]  Training Accuracy:0.500542888165038\n",
      "The training loss at 223th epoch : [0.78575548]  Training Accuracy:0.500542888165038\n",
      "The training loss at 224th epoch : [0.78556355]  Training Accuracy:0.500542888165038\n",
      "The training loss at 225th epoch : [0.78537199]  Training Accuracy:0.500542888165038\n",
      "The training loss at 226th epoch : [0.78518081]  Training Accuracy:0.500542888165038\n",
      "The training loss at 227th epoch : [0.78499]  Training Accuracy:0.500542888165038\n",
      "The training loss at 228th epoch : [0.78479957]  Training Accuracy:0.500542888165038\n",
      "The training loss at 229th epoch : [0.7846095]  Training Accuracy:0.500542888165038\n",
      "The training loss at 230th epoch : [0.78441981]  Training Accuracy:0.500542888165038\n",
      "The training loss at 231th epoch : [0.78423049]  Training Accuracy:0.500542888165038\n",
      "The training loss at 232th epoch : [0.78404153]  Training Accuracy:0.500542888165038\n",
      "The training loss at 233th epoch : [0.78385295]  Training Accuracy:0.500542888165038\n",
      "The training loss at 234th epoch : [0.78366474]  Training Accuracy:0.500542888165038\n",
      "The training loss at 235th epoch : [0.78347689]  Training Accuracy:0.500542888165038\n",
      "The training loss at 236th epoch : [0.78328941]  Training Accuracy:0.500542888165038\n",
      "The training loss at 237th epoch : [0.7831023]  Training Accuracy:0.500542888165038\n",
      "The training loss at 238th epoch : [0.78291556]  Training Accuracy:0.500542888165038\n",
      "The training loss at 239th epoch : [0.78272918]  Training Accuracy:0.500542888165038\n",
      "The training loss at 240th epoch : [0.78254316]  Training Accuracy:0.500542888165038\n",
      "The training loss at 241th epoch : [0.78235751]  Training Accuracy:0.500542888165038\n",
      "The training loss at 242th epoch : [0.78217223]  Training Accuracy:0.500542888165038\n",
      "The training loss at 243th epoch : [0.78198731]  Training Accuracy:0.500542888165038\n",
      "The training loss at 244th epoch : [0.78180274]  Training Accuracy:0.500542888165038\n",
      "The training loss at 245th epoch : [0.78161855]  Training Accuracy:0.500542888165038\n",
      "The training loss at 246th epoch : [0.78143471]  Training Accuracy:0.500542888165038\n",
      "The training loss at 247th epoch : [0.78125123]  Training Accuracy:0.500542888165038\n",
      "The training loss at 248th epoch : [0.78106812]  Training Accuracy:0.500542888165038\n",
      "The training loss at 249th epoch : [0.78088536]  Training Accuracy:0.500542888165038\n",
      "The training loss at 250th epoch : [0.78070296]  Training Accuracy:0.500542888165038\n",
      "The training loss at 251th epoch : [0.78052092]  Training Accuracy:0.500542888165038\n",
      "The training loss at 252th epoch : [0.78033924]  Training Accuracy:0.500542888165038\n",
      "The training loss at 253th epoch : [0.78015791]  Training Accuracy:0.500542888165038\n",
      "The training loss at 254th epoch : [0.77997694]  Training Accuracy:0.500542888165038\n",
      "The training loss at 255th epoch : [0.77979633]  Training Accuracy:0.500542888165038\n",
      "The training loss at 256th epoch : [0.77961607]  Training Accuracy:0.500542888165038\n",
      "The training loss at 257th epoch : [0.77943617]  Training Accuracy:0.500542888165038\n",
      "The training loss at 258th epoch : [0.77925662]  Training Accuracy:0.500542888165038\n",
      "The training loss at 259th epoch : [0.77907742]  Training Accuracy:0.500542888165038\n",
      "The training loss at 260th epoch : [0.77889857]  Training Accuracy:0.500542888165038\n",
      "The training loss at 261th epoch : [0.77872008]  Training Accuracy:0.500542888165038\n",
      "The training loss at 262th epoch : [0.77854194]  Training Accuracy:0.500542888165038\n",
      "The training loss at 263th epoch : [0.77836415]  Training Accuracy:0.500542888165038\n",
      "The training loss at 264th epoch : [0.77818671]  Training Accuracy:0.500542888165038\n",
      "The training loss at 265th epoch : [0.77800962]  Training Accuracy:0.500542888165038\n",
      "The training loss at 266th epoch : [0.77783288]  Training Accuracy:0.500542888165038\n",
      "The training loss at 267th epoch : [0.77765649]  Training Accuracy:0.500542888165038\n",
      "The training loss at 268th epoch : [0.77748044]  Training Accuracy:0.500542888165038\n",
      "The training loss at 269th epoch : [0.77730475]  Training Accuracy:0.500542888165038\n",
      "The training loss at 270th epoch : [0.77712939]  Training Accuracy:0.500542888165038\n",
      "The training loss at 271th epoch : [0.77695439]  Training Accuracy:0.500542888165038\n",
      "The training loss at 272th epoch : [0.77677973]  Training Accuracy:0.500542888165038\n",
      "The training loss at 273th epoch : [0.77660541]  Training Accuracy:0.500542888165038\n",
      "The training loss at 274th epoch : [0.77643144]  Training Accuracy:0.500542888165038\n",
      "The training loss at 275th epoch : [0.77625782]  Training Accuracy:0.500542888165038\n",
      "The training loss at 276th epoch : [0.77608453]  Training Accuracy:0.500542888165038\n",
      "The training loss at 277th epoch : [0.77591159]  Training Accuracy:0.500542888165038\n",
      "The training loss at 278th epoch : [0.77573899]  Training Accuracy:0.500542888165038\n",
      "The training loss at 279th epoch : [0.77556673]  Training Accuracy:0.500542888165038\n",
      "The training loss at 280th epoch : [0.77539481]  Training Accuracy:0.500542888165038\n",
      "The training loss at 281th epoch : [0.77522324]  Training Accuracy:0.500542888165038\n",
      "The training loss at 282th epoch : [0.775052]  Training Accuracy:0.500542888165038\n",
      "The training loss at 283th epoch : [0.7748811]  Training Accuracy:0.500542888165038\n",
      "The training loss at 284th epoch : [0.77471054]  Training Accuracy:0.500542888165038\n",
      "The training loss at 285th epoch : [0.77454031]  Training Accuracy:0.500542888165038\n",
      "The training loss at 286th epoch : [0.77437043]  Training Accuracy:0.500542888165038\n",
      "The training loss at 287th epoch : [0.77420088]  Training Accuracy:0.500542888165038\n",
      "The training loss at 288th epoch : [0.77403166]  Training Accuracy:0.500542888165038\n",
      "The training loss at 289th epoch : [0.77386278]  Training Accuracy:0.500542888165038\n",
      "The training loss at 290th epoch : [0.77369424]  Training Accuracy:0.500542888165038\n",
      "The training loss at 291th epoch : [0.77352603]  Training Accuracy:0.500542888165038\n",
      "The training loss at 292th epoch : [0.77335815]  Training Accuracy:0.500542888165038\n",
      "The training loss at 293th epoch : [0.7731906]  Training Accuracy:0.500542888165038\n",
      "The training loss at 294th epoch : [0.77302339]  Training Accuracy:0.500542888165038\n",
      "The training loss at 295th epoch : [0.77285651]  Training Accuracy:0.500542888165038\n",
      "The training loss at 296th epoch : [0.77268996]  Training Accuracy:0.500542888165038\n",
      "The training loss at 297th epoch : [0.77252375]  Training Accuracy:0.500542888165038\n",
      "The training loss at 298th epoch : [0.77235786]  Training Accuracy:0.500542888165038\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training loss at 299th epoch : [0.7721923]  Training Accuracy:0.500542888165038\n",
      "The training loss at 300th epoch : [0.77202707]  Training Accuracy:0.500542888165038\n",
      "The training loss at 301th epoch : [0.77186217]  Training Accuracy:0.500542888165038\n",
      "The training loss at 302th epoch : [0.77169759]  Training Accuracy:0.500542888165038\n",
      "The training loss at 303th epoch : [0.77153334]  Training Accuracy:0.500542888165038\n",
      "The training loss at 304th epoch : [0.77136942]  Training Accuracy:0.500542888165038\n",
      "The training loss at 305th epoch : [0.77120583]  Training Accuracy:0.500542888165038\n",
      "The training loss at 306th epoch : [0.77104256]  Training Accuracy:0.500542888165038\n",
      "The training loss at 307th epoch : [0.77087961]  Training Accuracy:0.500542888165038\n",
      "The training loss at 308th epoch : [0.77071699]  Training Accuracy:0.500542888165038\n",
      "The training loss at 309th epoch : [0.7705547]  Training Accuracy:0.500542888165038\n",
      "The training loss at 310th epoch : [0.77039272]  Training Accuracy:0.500542888165038\n",
      "The training loss at 311th epoch : [0.77023107]  Training Accuracy:0.500542888165038\n",
      "The training loss at 312th epoch : [0.77006974]  Training Accuracy:0.500542888165038\n",
      "The training loss at 313th epoch : [0.76990874]  Training Accuracy:0.500542888165038\n",
      "The training loss at 314th epoch : [0.76974805]  Training Accuracy:0.500542888165038\n",
      "The training loss at 315th epoch : [0.76958768]  Training Accuracy:0.500542888165038\n",
      "The training loss at 316th epoch : [0.76942763]  Training Accuracy:0.500542888165038\n",
      "The training loss at 317th epoch : [0.76926791]  Training Accuracy:0.500542888165038\n",
      "The training loss at 318th epoch : [0.7691085]  Training Accuracy:0.500542888165038\n",
      "The training loss at 319th epoch : [0.7689494]  Training Accuracy:0.500542888165038\n",
      "The training loss at 320th epoch : [0.76879063]  Training Accuracy:0.500542888165038\n",
      "The training loss at 321th epoch : [0.76863217]  Training Accuracy:0.500542888165038\n",
      "The training loss at 322th epoch : [0.76847403]  Training Accuracy:0.500542888165038\n",
      "The training loss at 323th epoch : [0.7683162]  Training Accuracy:0.500542888165038\n",
      "The training loss at 324th epoch : [0.76815869]  Training Accuracy:0.500542888165038\n",
      "The training loss at 325th epoch : [0.7680015]  Training Accuracy:0.500542888165038\n",
      "The training loss at 326th epoch : [0.76784461]  Training Accuracy:0.500542888165038\n",
      "The training loss at 327th epoch : [0.76768804]  Training Accuracy:0.500542888165038\n",
      "The training loss at 328th epoch : [0.76753179]  Training Accuracy:0.500542888165038\n",
      "The training loss at 329th epoch : [0.76737584]  Training Accuracy:0.500542888165038\n",
      "The training loss at 330th epoch : [0.76722021]  Training Accuracy:0.500542888165038\n",
      "The training loss at 331th epoch : [0.76706489]  Training Accuracy:0.500542888165038\n",
      "The training loss at 332th epoch : [0.76690988]  Training Accuracy:0.500542888165038\n",
      "The training loss at 333th epoch : [0.76675518]  Training Accuracy:0.500542888165038\n",
      "The training loss at 334th epoch : [0.76660079]  Training Accuracy:0.500542888165038\n",
      "The training loss at 335th epoch : [0.76644671]  Training Accuracy:0.500542888165038\n",
      "The training loss at 336th epoch : [0.76629293]  Training Accuracy:0.500542888165038\n",
      "The training loss at 337th epoch : [0.76613947]  Training Accuracy:0.500542888165038\n",
      "The training loss at 338th epoch : [0.76598631]  Training Accuracy:0.500542888165038\n",
      "The training loss at 339th epoch : [0.76583346]  Training Accuracy:0.500542888165038\n",
      "The training loss at 340th epoch : [0.76568091]  Training Accuracy:0.500542888165038\n",
      "The training loss at 341th epoch : [0.76552867]  Training Accuracy:0.500542888165038\n",
      "The training loss at 342th epoch : [0.76537674]  Training Accuracy:0.500542888165038\n",
      "The training loss at 343th epoch : [0.76522511]  Training Accuracy:0.500542888165038\n",
      "The training loss at 344th epoch : [0.76507378]  Training Accuracy:0.500542888165038\n",
      "The training loss at 345th epoch : [0.76492276]  Training Accuracy:0.500542888165038\n",
      "The training loss at 346th epoch : [0.76477204]  Training Accuracy:0.500542888165038\n",
      "The training loss at 347th epoch : [0.76462162]  Training Accuracy:0.500542888165038\n",
      "The training loss at 348th epoch : [0.7644715]  Training Accuracy:0.500542888165038\n",
      "The training loss at 349th epoch : [0.76432169]  Training Accuracy:0.500542888165038\n",
      "The training loss at 350th epoch : [0.76417218]  Training Accuracy:0.500542888165038\n",
      "The training loss at 351th epoch : [0.76402296]  Training Accuracy:0.500542888165038\n",
      "The training loss at 352th epoch : [0.76387405]  Training Accuracy:0.500542888165038\n",
      "The training loss at 353th epoch : [0.76372543]  Training Accuracy:0.500542888165038\n",
      "The training loss at 354th epoch : [0.76357712]  Training Accuracy:0.500542888165038\n",
      "The training loss at 355th epoch : [0.7634291]  Training Accuracy:0.500542888165038\n",
      "The training loss at 356th epoch : [0.76328138]  Training Accuracy:0.500542888165038\n",
      "The training loss at 357th epoch : [0.76313395]  Training Accuracy:0.500542888165038\n",
      "The training loss at 358th epoch : [0.76298682]  Training Accuracy:0.500542888165038\n",
      "The training loss at 359th epoch : [0.76283999]  Training Accuracy:0.500542888165038\n",
      "The training loss at 360th epoch : [0.76269346]  Training Accuracy:0.500542888165038\n",
      "The training loss at 361th epoch : [0.76254721]  Training Accuracy:0.500542888165038\n",
      "The training loss at 362th epoch : [0.76240127]  Training Accuracy:0.500542888165038\n",
      "The training loss at 363th epoch : [0.76225561]  Training Accuracy:0.500542888165038\n",
      "The training loss at 364th epoch : [0.76211025]  Training Accuracy:0.500542888165038\n",
      "The training loss at 365th epoch : [0.76196518]  Training Accuracy:0.500542888165038\n",
      "The training loss at 366th epoch : [0.76182041]  Training Accuracy:0.500542888165038\n",
      "The training loss at 367th epoch : [0.76167592]  Training Accuracy:0.500542888165038\n",
      "The training loss at 368th epoch : [0.76153173]  Training Accuracy:0.500542888165038\n",
      "The training loss at 369th epoch : [0.76138782]  Training Accuracy:0.500542888165038\n",
      "The training loss at 370th epoch : [0.76124421]  Training Accuracy:0.500542888165038\n",
      "The training loss at 371th epoch : [0.76110089]  Training Accuracy:0.500542888165038\n",
      "The training loss at 372th epoch : [0.76095785]  Training Accuracy:0.500542888165038\n",
      "The training loss at 373th epoch : [0.7608151]  Training Accuracy:0.500542888165038\n",
      "The training loss at 374th epoch : [0.76067265]  Training Accuracy:0.500542888165038\n",
      "The training loss at 375th epoch : [0.76053047]  Training Accuracy:0.500542888165038\n",
      "The training loss at 376th epoch : [0.76038859]  Training Accuracy:0.500542888165038\n",
      "The training loss at 377th epoch : [0.76024699]  Training Accuracy:0.500542888165038\n",
      "The training loss at 378th epoch : [0.76010568]  Training Accuracy:0.500542888165038\n",
      "The training loss at 379th epoch : [0.75996465]  Training Accuracy:0.500542888165038\n",
      "The training loss at 380th epoch : [0.75982391]  Training Accuracy:0.500542888165038\n",
      "The training loss at 381th epoch : [0.75968345]  Training Accuracy:0.500542888165038\n",
      "The training loss at 382th epoch : [0.75954327]  Training Accuracy:0.500542888165038\n",
      "The training loss at 383th epoch : [0.75940338]  Training Accuracy:0.500542888165038\n",
      "The training loss at 384th epoch : [0.75926377]  Training Accuracy:0.500542888165038\n",
      "The training loss at 385th epoch : [0.75912445]  Training Accuracy:0.500542888165038\n",
      "The training loss at 386th epoch : [0.7589854]  Training Accuracy:0.500542888165038\n",
      "The training loss at 387th epoch : [0.75884664]  Training Accuracy:0.500542888165038\n",
      "The training loss at 388th epoch : [0.75870815]  Training Accuracy:0.500542888165038\n",
      "The training loss at 389th epoch : [0.75856995]  Training Accuracy:0.500542888165038\n",
      "The training loss at 390th epoch : [0.75843202]  Training Accuracy:0.500542888165038\n",
      "The training loss at 391th epoch : [0.75829438]  Training Accuracy:0.500542888165038\n",
      "The training loss at 392th epoch : [0.75815701]  Training Accuracy:0.500542888165038\n",
      "The training loss at 393th epoch : [0.75801992]  Training Accuracy:0.500542888165038\n",
      "The training loss at 394th epoch : [0.75788311]  Training Accuracy:0.500542888165038\n",
      "The training loss at 395th epoch : [0.75774658]  Training Accuracy:0.500542888165038\n",
      "The training loss at 396th epoch : [0.75761032]  Training Accuracy:0.500542888165038\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training loss at 397th epoch : [0.75747434]  Training Accuracy:0.500542888165038\n",
      "The training loss at 398th epoch : [0.75733863]  Training Accuracy:0.500542888165038\n",
      "The training loss at 399th epoch : [0.7572032]  Training Accuracy:0.500542888165038\n",
      "The training loss at 400th epoch : [0.75706804]  Training Accuracy:0.500542888165038\n",
      "The training loss at 401th epoch : [0.75693316]  Training Accuracy:0.500542888165038\n",
      "The training loss at 402th epoch : [0.75679855]  Training Accuracy:0.500542888165038\n",
      "The training loss at 403th epoch : [0.75666421]  Training Accuracy:0.500542888165038\n",
      "The training loss at 404th epoch : [0.75653015]  Training Accuracy:0.500542888165038\n",
      "The training loss at 405th epoch : [0.75639635]  Training Accuracy:0.500542888165038\n",
      "The training loss at 406th epoch : [0.75626283]  Training Accuracy:0.500542888165038\n",
      "The training loss at 407th epoch : [0.75612958]  Training Accuracy:0.500542888165038\n",
      "The training loss at 408th epoch : [0.7559966]  Training Accuracy:0.500542888165038\n",
      "The training loss at 409th epoch : [0.75586389]  Training Accuracy:0.500542888165038\n",
      "The training loss at 410th epoch : [0.75573145]  Training Accuracy:0.500542888165038\n",
      "The training loss at 411th epoch : [0.75559928]  Training Accuracy:0.500542888165038\n",
      "The training loss at 412th epoch : [0.75546737]  Training Accuracy:0.500542888165038\n",
      "The training loss at 413th epoch : [0.75533574]  Training Accuracy:0.500542888165038\n",
      "The training loss at 414th epoch : [0.75520437]  Training Accuracy:0.500542888165038\n",
      "The training loss at 415th epoch : [0.75507326]  Training Accuracy:0.500542888165038\n",
      "The training loss at 416th epoch : [0.75494243]  Training Accuracy:0.500542888165038\n",
      "The training loss at 417th epoch : [0.75481186]  Training Accuracy:0.500542888165038\n",
      "The training loss at 418th epoch : [0.75468156]  Training Accuracy:0.500542888165038\n",
      "The training loss at 419th epoch : [0.75455152]  Training Accuracy:0.500542888165038\n",
      "The training loss at 420th epoch : [0.75442174]  Training Accuracy:0.500542888165038\n",
      "The training loss at 421th epoch : [0.75429223]  Training Accuracy:0.500542888165038\n",
      "The training loss at 422th epoch : [0.75416298]  Training Accuracy:0.500542888165038\n",
      "The training loss at 423th epoch : [0.754034]  Training Accuracy:0.500542888165038\n",
      "The training loss at 424th epoch : [0.75390528]  Training Accuracy:0.500542888165038\n",
      "The training loss at 425th epoch : [0.75377682]  Training Accuracy:0.500542888165038\n",
      "The training loss at 426th epoch : [0.75364862]  Training Accuracy:0.500542888165038\n",
      "The training loss at 427th epoch : [0.75352068]  Training Accuracy:0.500542888165038\n",
      "The training loss at 428th epoch : [0.753393]  Training Accuracy:0.500542888165038\n",
      "The training loss at 429th epoch : [0.75326558]  Training Accuracy:0.500542888165038\n",
      "The training loss at 430th epoch : [0.75313843]  Training Accuracy:0.500542888165038\n",
      "The training loss at 431th epoch : [0.75301153]  Training Accuracy:0.500542888165038\n",
      "The training loss at 432th epoch : [0.75288489]  Training Accuracy:0.500542888165038\n",
      "The training loss at 433th epoch : [0.75275851]  Training Accuracy:0.500542888165038\n",
      "The training loss at 434th epoch : [0.75263238]  Training Accuracy:0.500542888165038\n",
      "The training loss at 435th epoch : [0.75250651]  Training Accuracy:0.500542888165038\n",
      "The training loss at 436th epoch : [0.7523809]  Training Accuracy:0.500542888165038\n",
      "The training loss at 437th epoch : [0.75225555]  Training Accuracy:0.500542888165038\n",
      "The training loss at 438th epoch : [0.75213045]  Training Accuracy:0.500542888165038\n",
      "The training loss at 439th epoch : [0.75200561]  Training Accuracy:0.500542888165038\n",
      "The training loss at 440th epoch : [0.75188102]  Training Accuracy:0.500542888165038\n",
      "The training loss at 441th epoch : [0.75175668]  Training Accuracy:0.500542888165038\n",
      "The training loss at 442th epoch : [0.7516326]  Training Accuracy:0.500542888165038\n",
      "The training loss at 443th epoch : [0.75150877]  Training Accuracy:0.500542888165038\n",
      "The training loss at 444th epoch : [0.7513852]  Training Accuracy:0.500542888165038\n",
      "The training loss at 445th epoch : [0.75126187]  Training Accuracy:0.500542888165038\n",
      "The training loss at 446th epoch : [0.7511388]  Training Accuracy:0.500542888165038\n",
      "The training loss at 447th epoch : [0.75101598]  Training Accuracy:0.500542888165038\n",
      "The training loss at 448th epoch : [0.75089341]  Training Accuracy:0.500542888165038\n",
      "The training loss at 449th epoch : [0.7507711]  Training Accuracy:0.500542888165038\n",
      "The training loss at 450th epoch : [0.75064903]  Training Accuracy:0.500542888165038\n",
      "The training loss at 451th epoch : [0.75052721]  Training Accuracy:0.500542888165038\n",
      "The training loss at 452th epoch : [0.75040564]  Training Accuracy:0.500542888165038\n",
      "The training loss at 453th epoch : [0.75028432]  Training Accuracy:0.500542888165038\n",
      "The training loss at 454th epoch : [0.75016324]  Training Accuracy:0.500542888165038\n",
      "The training loss at 455th epoch : [0.75004242]  Training Accuracy:0.500542888165038\n",
      "The training loss at 456th epoch : [0.74992184]  Training Accuracy:0.500542888165038\n",
      "The training loss at 457th epoch : [0.74980151]  Training Accuracy:0.500542888165038\n",
      "The training loss at 458th epoch : [0.74968142]  Training Accuracy:0.500542888165038\n",
      "The training loss at 459th epoch : [0.74956158]  Training Accuracy:0.500542888165038\n",
      "The training loss at 460th epoch : [0.74944199]  Training Accuracy:0.500542888165038\n",
      "The training loss at 461th epoch : [0.74932264]  Training Accuracy:0.500542888165038\n",
      "The training loss at 462th epoch : [0.74920353]  Training Accuracy:0.500542888165038\n",
      "The training loss at 463th epoch : [0.74908467]  Training Accuracy:0.500542888165038\n",
      "The training loss at 464th epoch : [0.74896605]  Training Accuracy:0.500542888165038\n",
      "The training loss at 465th epoch : [0.74884768]  Training Accuracy:0.500542888165038\n",
      "The training loss at 466th epoch : [0.74872954]  Training Accuracy:0.500542888165038\n",
      "The training loss at 467th epoch : [0.74861165]  Training Accuracy:0.500542888165038\n",
      "The training loss at 468th epoch : [0.74849401]  Training Accuracy:0.500542888165038\n",
      "The training loss at 469th epoch : [0.7483766]  Training Accuracy:0.500542888165038\n",
      "The training loss at 470th epoch : [0.74825943]  Training Accuracy:0.500542888165038\n",
      "The training loss at 471th epoch : [0.7481425]  Training Accuracy:0.500542888165038\n",
      "The training loss at 472th epoch : [0.74802582]  Training Accuracy:0.500542888165038\n",
      "The training loss at 473th epoch : [0.74790937]  Training Accuracy:0.500542888165038\n",
      "The training loss at 474th epoch : [0.74779316]  Training Accuracy:0.500542888165038\n",
      "The training loss at 475th epoch : [0.74767719]  Training Accuracy:0.500542888165038\n",
      "The training loss at 476th epoch : [0.74756146]  Training Accuracy:0.500542888165038\n",
      "The training loss at 477th epoch : [0.74744596]  Training Accuracy:0.500542888165038\n",
      "The training loss at 478th epoch : [0.7473307]  Training Accuracy:0.500542888165038\n",
      "The training loss at 479th epoch : [0.74721568]  Training Accuracy:0.500542888165038\n",
      "The training loss at 480th epoch : [0.7471009]  Training Accuracy:0.500542888165038\n",
      "The training loss at 481th epoch : [0.74698635]  Training Accuracy:0.500542888165038\n",
      "The training loss at 482th epoch : [0.74687203]  Training Accuracy:0.500542888165038\n",
      "The training loss at 483th epoch : [0.74675795]  Training Accuracy:0.500542888165038\n",
      "The training loss at 484th epoch : [0.74664411]  Training Accuracy:0.500542888165038\n",
      "The training loss at 485th epoch : [0.74653049]  Training Accuracy:0.500542888165038\n",
      "The training loss at 486th epoch : [0.74641712]  Training Accuracy:0.500542888165038\n",
      "The training loss at 487th epoch : [0.74630397]  Training Accuracy:0.500542888165038\n",
      "The training loss at 488th epoch : [0.74619106]  Training Accuracy:0.500542888165038\n",
      "The training loss at 489th epoch : [0.74607838]  Training Accuracy:0.500542888165038\n",
      "The training loss at 490th epoch : [0.74596593]  Training Accuracy:0.500542888165038\n",
      "The training loss at 491th epoch : [0.74585371]  Training Accuracy:0.500542888165038\n",
      "The training loss at 492th epoch : [0.74574173]  Training Accuracy:0.500542888165038\n",
      "The training loss at 493th epoch : [0.74562997]  Training Accuracy:0.500542888165038\n",
      "The training loss at 494th epoch : [0.74551844]  Training Accuracy:0.500542888165038\n",
      "The training loss at 495th epoch : [0.74540715]  Training Accuracy:0.500542888165038\n",
      "The training loss at 496th epoch : [0.74529608]  Training Accuracy:0.500542888165038\n",
      "The training loss at 497th epoch : [0.74518524]  Training Accuracy:0.500542888165038\n",
      "The training loss at 498th epoch : [0.74507463]  Training Accuracy:0.500542888165038\n",
      "The training loss at 499th epoch : [0.74496424]  Training Accuracy:0.500542888165038\n",
      "The training loss at 500th epoch : [0.74485409]  Training Accuracy:0.500542888165038\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training loss at 501th epoch : [0.74474416]  Training Accuracy:0.500542888165038\n",
      "The training loss at 502th epoch : [0.74463446]  Training Accuracy:0.500542888165038\n",
      "The training loss at 503th epoch : [0.74452498]  Training Accuracy:0.500542888165038\n",
      "The training loss at 504th epoch : [0.74441573]  Training Accuracy:0.500542888165038\n",
      "The training loss at 505th epoch : [0.7443067]  Training Accuracy:0.500542888165038\n",
      "The training loss at 506th epoch : [0.7441979]  Training Accuracy:0.500542888165038\n",
      "The training loss at 507th epoch : [0.74408932]  Training Accuracy:0.500542888165038\n",
      "The training loss at 508th epoch : [0.74398097]  Training Accuracy:0.500542888165038\n",
      "The training loss at 509th epoch : [0.74387284]  Training Accuracy:0.500542888165038\n",
      "The training loss at 510th epoch : [0.74376494]  Training Accuracy:0.500542888165038\n",
      "The training loss at 511th epoch : [0.74365725]  Training Accuracy:0.500542888165038\n",
      "The training loss at 512th epoch : [0.74354979]  Training Accuracy:0.500542888165038\n",
      "The training loss at 513th epoch : [0.74344255]  Training Accuracy:0.500542888165038\n",
      "The training loss at 514th epoch : [0.74333553]  Training Accuracy:0.500542888165038\n",
      "The training loss at 515th epoch : [0.74322873]  Training Accuracy:0.500542888165038\n",
      "The training loss at 516th epoch : [0.74312215]  Training Accuracy:0.500542888165038\n",
      "The training loss at 517th epoch : [0.7430158]  Training Accuracy:0.500542888165038\n",
      "The training loss at 518th epoch : [0.74290966]  Training Accuracy:0.500542888165038\n",
      "The training loss at 519th epoch : [0.74280374]  Training Accuracy:0.500542888165038\n",
      "The training loss at 520th epoch : [0.74269804]  Training Accuracy:0.500542888165038\n",
      "The training loss at 521th epoch : [0.74259255]  Training Accuracy:0.500542888165038\n",
      "The training loss at 522th epoch : [0.74248729]  Training Accuracy:0.500542888165038\n",
      "The training loss at 523th epoch : [0.74238224]  Training Accuracy:0.500542888165038\n",
      "The training loss at 524th epoch : [0.74227741]  Training Accuracy:0.500542888165038\n",
      "The training loss at 525th epoch : [0.7421728]  Training Accuracy:0.500542888165038\n",
      "The training loss at 526th epoch : [0.7420684]  Training Accuracy:0.500542888165038\n",
      "The training loss at 527th epoch : [0.74196422]  Training Accuracy:0.500542888165038\n",
      "The training loss at 528th epoch : [0.74186026]  Training Accuracy:0.500542888165038\n",
      "The training loss at 529th epoch : [0.74175651]  Training Accuracy:0.500542888165038\n",
      "The training loss at 530th epoch : [0.74165297]  Training Accuracy:0.500542888165038\n",
      "The training loss at 531th epoch : [0.74154965]  Training Accuracy:0.500542888165038\n",
      "The training loss at 532th epoch : [0.74144654]  Training Accuracy:0.500542888165038\n",
      "The training loss at 533th epoch : [0.74134364]  Training Accuracy:0.500542888165038\n",
      "The training loss at 534th epoch : [0.74124096]  Training Accuracy:0.500542888165038\n",
      "The training loss at 535th epoch : [0.74113849]  Training Accuracy:0.500542888165038\n",
      "The training loss at 536th epoch : [0.74103623]  Training Accuracy:0.500542888165038\n",
      "The training loss at 537th epoch : [0.74093419]  Training Accuracy:0.500542888165038\n",
      "The training loss at 538th epoch : [0.74083235]  Training Accuracy:0.500542888165038\n",
      "The training loss at 539th epoch : [0.74073073]  Training Accuracy:0.500542888165038\n",
      "The training loss at 540th epoch : [0.74062932]  Training Accuracy:0.500542888165038\n",
      "The training loss at 541th epoch : [0.74052811]  Training Accuracy:0.500542888165038\n",
      "The training loss at 542th epoch : [0.74042712]  Training Accuracy:0.500542888165038\n",
      "The training loss at 543th epoch : [0.74032633]  Training Accuracy:0.500542888165038\n",
      "The training loss at 544th epoch : [0.74022576]  Training Accuracy:0.500542888165038\n",
      "The training loss at 545th epoch : [0.74012539]  Training Accuracy:0.500542888165038\n",
      "The training loss at 546th epoch : [0.74002523]  Training Accuracy:0.500542888165038\n",
      "The training loss at 547th epoch : [0.73992528]  Training Accuracy:0.500542888165038\n",
      "The training loss at 548th epoch : [0.73982553]  Training Accuracy:0.500542888165038\n",
      "The training loss at 549th epoch : [0.739726]  Training Accuracy:0.500542888165038\n",
      "The training loss at 550th epoch : [0.73962666]  Training Accuracy:0.500542888165038\n",
      "The training loss at 551th epoch : [0.73952754]  Training Accuracy:0.500542888165038\n",
      "The training loss at 552th epoch : [0.73942862]  Training Accuracy:0.500542888165038\n",
      "The training loss at 553th epoch : [0.7393299]  Training Accuracy:0.500542888165038\n",
      "The training loss at 554th epoch : [0.73923139]  Training Accuracy:0.500542888165038\n",
      "The training loss at 555th epoch : [0.73913309]  Training Accuracy:0.500542888165038\n",
      "The training loss at 556th epoch : [0.73903499]  Training Accuracy:0.500542888165038\n",
      "The training loss at 557th epoch : [0.73893709]  Training Accuracy:0.500542888165038\n",
      "The training loss at 558th epoch : [0.73883939]  Training Accuracy:0.500542888165038\n",
      "The training loss at 559th epoch : [0.7387419]  Training Accuracy:0.500542888165038\n",
      "The training loss at 560th epoch : [0.73864461]  Training Accuracy:0.500542888165038\n",
      "The training loss at 561th epoch : [0.73854752]  Training Accuracy:0.500542888165038\n",
      "The training loss at 562th epoch : [0.73845064]  Training Accuracy:0.500542888165038\n",
      "The training loss at 563th epoch : [0.73835395]  Training Accuracy:0.500542888165038\n",
      "The training loss at 564th epoch : [0.73825747]  Training Accuracy:0.500542888165038\n",
      "The training loss at 565th epoch : [0.73816119]  Training Accuracy:0.500542888165038\n",
      "The training loss at 566th epoch : [0.7380651]  Training Accuracy:0.500542888165038\n",
      "The training loss at 567th epoch : [0.73796922]  Training Accuracy:0.500542888165038\n",
      "The training loss at 568th epoch : [0.73787353]  Training Accuracy:0.500542888165038\n",
      "The training loss at 569th epoch : [0.73777805]  Training Accuracy:0.500542888165038\n",
      "The training loss at 570th epoch : [0.73768276]  Training Accuracy:0.500542888165038\n",
      "The training loss at 571th epoch : [0.73758767]  Training Accuracy:0.500542888165038\n",
      "The training loss at 572th epoch : [0.73749278]  Training Accuracy:0.500542888165038\n",
      "The training loss at 573th epoch : [0.73739809]  Training Accuracy:0.500542888165038\n",
      "The training loss at 574th epoch : [0.73730359]  Training Accuracy:0.500542888165038\n",
      "The training loss at 575th epoch : [0.73720929]  Training Accuracy:0.500542888165038\n",
      "The training loss at 576th epoch : [0.73711518]  Training Accuracy:0.500542888165038\n",
      "The training loss at 577th epoch : [0.73702128]  Training Accuracy:0.500542888165038\n",
      "The training loss at 578th epoch : [0.73692756]  Training Accuracy:0.500542888165038\n",
      "The training loss at 579th epoch : [0.73683404]  Training Accuracy:0.500542888165038\n",
      "The training loss at 580th epoch : [0.73674072]  Training Accuracy:0.500542888165038\n",
      "The training loss at 581th epoch : [0.73664759]  Training Accuracy:0.500542888165038\n",
      "The training loss at 582th epoch : [0.73655466]  Training Accuracy:0.500542888165038\n",
      "The training loss at 583th epoch : [0.73646191]  Training Accuracy:0.500542888165038\n",
      "The training loss at 584th epoch : [0.73636937]  Training Accuracy:0.500542888165038\n",
      "The training loss at 585th epoch : [0.73627701]  Training Accuracy:0.500542888165038\n",
      "The training loss at 586th epoch : [0.73618485]  Training Accuracy:0.500542888165038\n",
      "The training loss at 587th epoch : [0.73609287]  Training Accuracy:0.500542888165038\n",
      "The training loss at 588th epoch : [0.73600109]  Training Accuracy:0.500542888165038\n",
      "The training loss at 589th epoch : [0.7359095]  Training Accuracy:0.500542888165038\n",
      "The training loss at 590th epoch : [0.73581811]  Training Accuracy:0.500542888165038\n",
      "The training loss at 591th epoch : [0.7357269]  Training Accuracy:0.500542888165038\n",
      "The training loss at 592th epoch : [0.73563588]  Training Accuracy:0.500542888165038\n",
      "The training loss at 593th epoch : [0.73554505]  Training Accuracy:0.500542888165038\n",
      "The training loss at 594th epoch : [0.73545442]  Training Accuracy:0.500542888165038\n",
      "The training loss at 595th epoch : [0.73536397]  Training Accuracy:0.500542888165038\n",
      "The training loss at 596th epoch : [0.73527371]  Training Accuracy:0.500542888165038\n",
      "The training loss at 597th epoch : [0.73518363]  Training Accuracy:0.500542888165038\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training loss at 598th epoch : [0.73509375]  Training Accuracy:0.500542888165038\n",
      "The training loss at 599th epoch : [0.73500405]  Training Accuracy:0.500542888165038\n",
      "The training loss at 600th epoch : [0.73491454]  Training Accuracy:0.500542888165038\n",
      "The training loss at 601th epoch : [0.73482522]  Training Accuracy:0.500542888165038\n",
      "The training loss at 602th epoch : [0.73473608]  Training Accuracy:0.500542888165038\n",
      "The training loss at 603th epoch : [0.73464713]  Training Accuracy:0.500542888165038\n",
      "The training loss at 604th epoch : [0.73455836]  Training Accuracy:0.500542888165038\n",
      "The training loss at 605th epoch : [0.73446978]  Training Accuracy:0.500542888165038\n",
      "The training loss at 606th epoch : [0.73438139]  Training Accuracy:0.500542888165038\n",
      "The training loss at 607th epoch : [0.73429318]  Training Accuracy:0.500542888165038\n",
      "The training loss at 608th epoch : [0.73420515]  Training Accuracy:0.500542888165038\n",
      "The training loss at 609th epoch : [0.73411731]  Training Accuracy:0.500542888165038\n",
      "The training loss at 610th epoch : [0.73402965]  Training Accuracy:0.500542888165038\n",
      "The training loss at 611th epoch : [0.73394218]  Training Accuracy:0.500542888165038\n",
      "The training loss at 612th epoch : [0.73385488]  Training Accuracy:0.500542888165038\n",
      "The training loss at 613th epoch : [0.73376777]  Training Accuracy:0.500542888165038\n",
      "The training loss at 614th epoch : [0.73368084]  Training Accuracy:0.500542888165038\n",
      "The training loss at 615th epoch : [0.7335941]  Training Accuracy:0.500542888165038\n",
      "The training loss at 616th epoch : [0.73350753]  Training Accuracy:0.500542888165038\n",
      "The training loss at 617th epoch : [0.73342115]  Training Accuracy:0.500542888165038\n",
      "The training loss at 618th epoch : [0.73333494]  Training Accuracy:0.500542888165038\n",
      "The training loss at 619th epoch : [0.73324892]  Training Accuracy:0.500542888165038\n",
      "The training loss at 620th epoch : [0.73316308]  Training Accuracy:0.500542888165038\n",
      "The training loss at 621th epoch : [0.73307741]  Training Accuracy:0.500542888165038\n",
      "The training loss at 622th epoch : [0.73299193]  Training Accuracy:0.500542888165038\n",
      "The training loss at 623th epoch : [0.73290662]  Training Accuracy:0.500542888165038\n",
      "The training loss at 624th epoch : [0.73282149]  Training Accuracy:0.500542888165038\n",
      "The training loss at 625th epoch : [0.73273654]  Training Accuracy:0.500542888165038\n",
      "The training loss at 626th epoch : [0.73265177]  Training Accuracy:0.500542888165038\n",
      "The training loss at 627th epoch : [0.73256718]  Training Accuracy:0.500542888165038\n",
      "The training loss at 628th epoch : [0.73248276]  Training Accuracy:0.500542888165038\n",
      "The training loss at 629th epoch : [0.73239852]  Training Accuracy:0.500542888165038\n",
      "The training loss at 630th epoch : [0.73231445]  Training Accuracy:0.500542888165038\n",
      "The training loss at 631th epoch : [0.73223056]  Training Accuracy:0.500542888165038\n",
      "The training loss at 632th epoch : [0.73214685]  Training Accuracy:0.500542888165038\n",
      "The training loss at 633th epoch : [0.73206331]  Training Accuracy:0.500542888165038\n",
      "The training loss at 634th epoch : [0.73197995]  Training Accuracy:0.500542888165038\n",
      "The training loss at 635th epoch : [0.73189676]  Training Accuracy:0.500542888165038\n",
      "The training loss at 636th epoch : [0.73181375]  Training Accuracy:0.500542888165038\n",
      "The training loss at 637th epoch : [0.73173091]  Training Accuracy:0.500542888165038\n",
      "The training loss at 638th epoch : [0.73164824]  Training Accuracy:0.500542888165038\n",
      "The training loss at 639th epoch : [0.73156575]  Training Accuracy:0.500542888165038\n",
      "The training loss at 640th epoch : [0.73148343]  Training Accuracy:0.500542888165038\n",
      "The training loss at 641th epoch : [0.73140128]  Training Accuracy:0.500542888165038\n",
      "The training loss at 642th epoch : [0.73131931]  Training Accuracy:0.500542888165038\n",
      "The training loss at 643th epoch : [0.7312375]  Training Accuracy:0.500542888165038\n",
      "The training loss at 644th epoch : [0.73115587]  Training Accuracy:0.500542888165038\n",
      "The training loss at 645th epoch : [0.73107441]  Training Accuracy:0.500542888165038\n",
      "The training loss at 646th epoch : [0.73099312]  Training Accuracy:0.500542888165038\n",
      "The training loss at 647th epoch : [0.730912]  Training Accuracy:0.500542888165038\n",
      "The training loss at 648th epoch : [0.73083105]  Training Accuracy:0.500542888165038\n",
      "The training loss at 649th epoch : [0.73075027]  Training Accuracy:0.500542888165038\n",
      "The training loss at 650th epoch : [0.73066966]  Training Accuracy:0.500542888165038\n",
      "The training loss at 651th epoch : [0.73058922]  Training Accuracy:0.500542888165038\n",
      "The training loss at 652th epoch : [0.73050895]  Training Accuracy:0.500542888165038\n",
      "The training loss at 653th epoch : [0.73042884]  Training Accuracy:0.500542888165038\n",
      "The training loss at 654th epoch : [0.73034891]  Training Accuracy:0.500542888165038\n",
      "The training loss at 655th epoch : [0.73026914]  Training Accuracy:0.500542888165038\n",
      "The training loss at 656th epoch : [0.73018954]  Training Accuracy:0.500542888165038\n",
      "The training loss at 657th epoch : [0.73011011]  Training Accuracy:0.500542888165038\n",
      "The training loss at 658th epoch : [0.73003084]  Training Accuracy:0.500542888165038\n",
      "The training loss at 659th epoch : [0.72995174]  Training Accuracy:0.500542888165038\n",
      "The training loss at 660th epoch : [0.72987281]  Training Accuracy:0.500542888165038\n",
      "The training loss at 661th epoch : [0.72979404]  Training Accuracy:0.500542888165038\n",
      "The training loss at 662th epoch : [0.72971543]  Training Accuracy:0.500542888165038\n",
      "The training loss at 663th epoch : [0.729637]  Training Accuracy:0.500542888165038\n",
      "The training loss at 664th epoch : [0.72955872]  Training Accuracy:0.500542888165038\n",
      "The training loss at 665th epoch : [0.72948062]  Training Accuracy:0.500542888165038\n",
      "The training loss at 666th epoch : [0.72940267]  Training Accuracy:0.500542888165038\n",
      "The training loss at 667th epoch : [0.72932489]  Training Accuracy:0.500542888165038\n",
      "The training loss at 668th epoch : [0.72924727]  Training Accuracy:0.500542888165038\n",
      "The training loss at 669th epoch : [0.72916982]  Training Accuracy:0.500542888165038\n",
      "The training loss at 670th epoch : [0.72909253]  Training Accuracy:0.500542888165038\n",
      "The training loss at 671th epoch : [0.7290154]  Training Accuracy:0.500542888165038\n",
      "The training loss at 672th epoch : [0.72893843]  Training Accuracy:0.500542888165038\n",
      "The training loss at 673th epoch : [0.72886163]  Training Accuracy:0.500542888165038\n",
      "The training loss at 674th epoch : [0.72878499]  Training Accuracy:0.500542888165038\n",
      "The training loss at 675th epoch : [0.7287085]  Training Accuracy:0.500542888165038\n",
      "The training loss at 676th epoch : [0.72863218]  Training Accuracy:0.500542888165038\n",
      "The training loss at 677th epoch : [0.72855602]  Training Accuracy:0.500542888165038\n",
      "The training loss at 678th epoch : [0.72848002]  Training Accuracy:0.500542888165038\n",
      "The training loss at 679th epoch : [0.72840418]  Training Accuracy:0.500542888165038\n",
      "The training loss at 680th epoch : [0.7283285]  Training Accuracy:0.500542888165038\n",
      "The training loss at 681th epoch : [0.72825298]  Training Accuracy:0.500542888165038\n",
      "The training loss at 682th epoch : [0.72817762]  Training Accuracy:0.500542888165038\n",
      "The training loss at 683th epoch : [0.72810241]  Training Accuracy:0.500542888165038\n",
      "The training loss at 684th epoch : [0.72802737]  Training Accuracy:0.500542888165038\n",
      "The training loss at 685th epoch : [0.72795248]  Training Accuracy:0.500542888165038\n",
      "The training loss at 686th epoch : [0.72787775]  Training Accuracy:0.500542888165038\n",
      "The training loss at 687th epoch : [0.72780318]  Training Accuracy:0.500542888165038\n",
      "The training loss at 688th epoch : [0.72772876]  Training Accuracy:0.500542888165038\n",
      "The training loss at 689th epoch : [0.72765451]  Training Accuracy:0.500542888165038\n",
      "The training loss at 690th epoch : [0.7275804]  Training Accuracy:0.500542888165038\n",
      "The training loss at 691th epoch : [0.72750646]  Training Accuracy:0.500542888165038\n",
      "The training loss at 692th epoch : [0.72743267]  Training Accuracy:0.500542888165038\n",
      "The training loss at 693th epoch : [0.72735903]  Training Accuracy:0.500542888165038\n",
      "The training loss at 694th epoch : [0.72728555]  Training Accuracy:0.500542888165038\n",
      "The training loss at 695th epoch : [0.72721223]  Training Accuracy:0.500542888165038\n",
      "The training loss at 696th epoch : [0.72713906]  Training Accuracy:0.500542888165038\n",
      "The training loss at 697th epoch : [0.72706604]  Training Accuracy:0.500542888165038\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training loss at 698th epoch : [0.72699318]  Training Accuracy:0.500542888165038\n",
      "The training loss at 699th epoch : [0.72692047]  Training Accuracy:0.500542888165038\n",
      "The training loss at 700th epoch : [0.72684792]  Training Accuracy:0.500542888165038\n",
      "The training loss at 701th epoch : [0.72677551]  Training Accuracy:0.500542888165038\n",
      "The training loss at 702th epoch : [0.72670326]  Training Accuracy:0.500542888165038\n",
      "The training loss at 703th epoch : [0.72663117]  Training Accuracy:0.500542888165038\n",
      "The training loss at 704th epoch : [0.72655922]  Training Accuracy:0.500542888165038\n",
      "The training loss at 705th epoch : [0.72648743]  Training Accuracy:0.500542888165038\n",
      "The training loss at 706th epoch : [0.72641579]  Training Accuracy:0.500542888165038\n",
      "The training loss at 707th epoch : [0.72634429]  Training Accuracy:0.500542888165038\n",
      "The training loss at 708th epoch : [0.72627295]  Training Accuracy:0.500542888165038\n",
      "The training loss at 709th epoch : [0.72620177]  Training Accuracy:0.500542888165038\n",
      "The training loss at 710th epoch : [0.72613073]  Training Accuracy:0.500542888165038\n",
      "The training loss at 711th epoch : [0.72605984]  Training Accuracy:0.500542888165038\n",
      "The training loss at 712th epoch : [0.7259891]  Training Accuracy:0.500542888165038\n",
      "The training loss at 713th epoch : [0.72591851]  Training Accuracy:0.500542888165038\n",
      "The training loss at 714th epoch : [0.72584807]  Training Accuracy:0.500542888165038\n",
      "The training loss at 715th epoch : [0.72577777]  Training Accuracy:0.500542888165038\n",
      "The training loss at 716th epoch : [0.72570763]  Training Accuracy:0.500542888165038\n",
      "The training loss at 717th epoch : [0.72563763]  Training Accuracy:0.500542888165038\n",
      "The training loss at 718th epoch : [0.72556778]  Training Accuracy:0.500542888165038\n",
      "The training loss at 719th epoch : [0.72549808]  Training Accuracy:0.500542888165038\n",
      "The training loss at 720th epoch : [0.72542853]  Training Accuracy:0.500542888165038\n",
      "The training loss at 721th epoch : [0.72535912]  Training Accuracy:0.500542888165038\n",
      "The training loss at 722th epoch : [0.72528986]  Training Accuracy:0.500542888165038\n",
      "The training loss at 723th epoch : [0.72522075]  Training Accuracy:0.500542888165038\n",
      "The training loss at 724th epoch : [0.72515178]  Training Accuracy:0.500542888165038\n",
      "The training loss at 725th epoch : [0.72508296]  Training Accuracy:0.500542888165038\n",
      "The training loss at 726th epoch : [0.72501428]  Training Accuracy:0.500542888165038\n",
      "The training loss at 727th epoch : [0.72494575]  Training Accuracy:0.500542888165038\n",
      "The training loss at 728th epoch : [0.72487737]  Training Accuracy:0.500542888165038\n",
      "The training loss at 729th epoch : [0.72480913]  Training Accuracy:0.500542888165038\n",
      "The training loss at 730th epoch : [0.72474103]  Training Accuracy:0.500542888165038\n",
      "The training loss at 731th epoch : [0.72467307]  Training Accuracy:0.500542888165038\n",
      "The training loss at 732th epoch : [0.72460526]  Training Accuracy:0.500542888165038\n",
      "The training loss at 733th epoch : [0.7245376]  Training Accuracy:0.500542888165038\n",
      "The training loss at 734th epoch : [0.72447008]  Training Accuracy:0.500542888165038\n",
      "The training loss at 735th epoch : [0.72440269]  Training Accuracy:0.500542888165038\n",
      "The training loss at 736th epoch : [0.72433546]  Training Accuracy:0.500542888165038\n",
      "The training loss at 737th epoch : [0.72426836]  Training Accuracy:0.500542888165038\n",
      "The training loss at 738th epoch : [0.72420141]  Training Accuracy:0.500542888165038\n",
      "The training loss at 739th epoch : [0.7241346]  Training Accuracy:0.500542888165038\n",
      "The training loss at 740th epoch : [0.72406793]  Training Accuracy:0.500542888165038\n",
      "The training loss at 741th epoch : [0.7240014]  Training Accuracy:0.500542888165038\n",
      "The training loss at 742th epoch : [0.72393501]  Training Accuracy:0.500542888165038\n",
      "The training loss at 743th epoch : [0.72386876]  Training Accuracy:0.500542888165038\n",
      "The training loss at 744th epoch : [0.72380265]  Training Accuracy:0.500542888165038\n",
      "The training loss at 745th epoch : [0.72373668]  Training Accuracy:0.500542888165038\n",
      "The training loss at 746th epoch : [0.72367086]  Training Accuracy:0.500542888165038\n",
      "The training loss at 747th epoch : [0.72360517]  Training Accuracy:0.500542888165038\n",
      "The training loss at 748th epoch : [0.72353962]  Training Accuracy:0.500542888165038\n",
      "The training loss at 749th epoch : [0.72347421]  Training Accuracy:0.500542888165038\n",
      "The training loss at 750th epoch : [0.72340893]  Training Accuracy:0.500542888165038\n",
      "The training loss at 751th epoch : [0.7233438]  Training Accuracy:0.500542888165038\n",
      "The training loss at 752th epoch : [0.7232788]  Training Accuracy:0.500542888165038\n",
      "The training loss at 753th epoch : [0.72321394]  Training Accuracy:0.500542888165038\n",
      "The training loss at 754th epoch : [0.72314922]  Training Accuracy:0.500542888165038\n",
      "The training loss at 755th epoch : [0.72308464]  Training Accuracy:0.500542888165038\n",
      "The training loss at 756th epoch : [0.72302019]  Training Accuracy:0.500542888165038\n",
      "The training loss at 757th epoch : [0.72295588]  Training Accuracy:0.500542888165038\n",
      "The training loss at 758th epoch : [0.72289171]  Training Accuracy:0.500542888165038\n",
      "The training loss at 759th epoch : [0.72282767]  Training Accuracy:0.500542888165038\n",
      "The training loss at 760th epoch : [0.72276377]  Training Accuracy:0.500542888165038\n",
      "The training loss at 761th epoch : [0.7227]  Training Accuracy:0.500542888165038\n",
      "The training loss at 762th epoch : [0.72263637]  Training Accuracy:0.500542888165038\n",
      "The training loss at 763th epoch : [0.72257287]  Training Accuracy:0.500542888165038\n",
      "The training loss at 764th epoch : [0.72250951]  Training Accuracy:0.500542888165038\n",
      "The training loss at 765th epoch : [0.72244628]  Training Accuracy:0.500542888165038\n",
      "The training loss at 766th epoch : [0.72238319]  Training Accuracy:0.500542888165038\n",
      "The training loss at 767th epoch : [0.72232023]  Training Accuracy:0.500542888165038\n",
      "The training loss at 768th epoch : [0.7222574]  Training Accuracy:0.500542888165038\n",
      "The training loss at 769th epoch : [0.72219471]  Training Accuracy:0.500542888165038\n",
      "The training loss at 770th epoch : [0.72213215]  Training Accuracy:0.500542888165038\n",
      "The training loss at 771th epoch : [0.72206972]  Training Accuracy:0.500542888165038\n",
      "The training loss at 772th epoch : [0.72200742]  Training Accuracy:0.500542888165038\n",
      "The training loss at 773th epoch : [0.72194526]  Training Accuracy:0.500542888165038\n",
      "The training loss at 774th epoch : [0.72188323]  Training Accuracy:0.500542888165038\n",
      "The training loss at 775th epoch : [0.72182133]  Training Accuracy:0.500542888165038\n",
      "The training loss at 776th epoch : [0.72175956]  Training Accuracy:0.500542888165038\n",
      "The training loss at 777th epoch : [0.72169793]  Training Accuracy:0.500542888165038\n",
      "The training loss at 778th epoch : [0.72163642]  Training Accuracy:0.500542888165038\n",
      "The training loss at 779th epoch : [0.72157504]  Training Accuracy:0.500542888165038\n",
      "The training loss at 780th epoch : [0.7215138]  Training Accuracy:0.500542888165038\n",
      "The training loss at 781th epoch : [0.72145268]  Training Accuracy:0.500542888165038\n",
      "The training loss at 782th epoch : [0.7213917]  Training Accuracy:0.500542888165038\n",
      "The training loss at 783th epoch : [0.72133084]  Training Accuracy:0.500542888165038\n",
      "The training loss at 784th epoch : [0.72127011]  Training Accuracy:0.500542888165038\n",
      "The training loss at 785th epoch : [0.72120952]  Training Accuracy:0.500542888165038\n",
      "The training loss at 786th epoch : [0.72114905]  Training Accuracy:0.500542888165038\n",
      "The training loss at 787th epoch : [0.72108871]  Training Accuracy:0.500542888165038\n",
      "The training loss at 788th epoch : [0.7210285]  Training Accuracy:0.500542888165038\n",
      "The training loss at 789th epoch : [0.72096841]  Training Accuracy:0.500542888165038\n",
      "The training loss at 790th epoch : [0.72090845]  Training Accuracy:0.500542888165038\n",
      "The training loss at 791th epoch : [0.72084862]  Training Accuracy:0.500542888165038\n",
      "The training loss at 792th epoch : [0.72078892]  Training Accuracy:0.500542888165038\n",
      "The training loss at 793th epoch : [0.72072935]  Training Accuracy:0.500542888165038\n",
      "The training loss at 794th epoch : [0.7206699]  Training Accuracy:0.500542888165038\n",
      "The training loss at 795th epoch : [0.72061058]  Training Accuracy:0.500542888165038\n",
      "The training loss at 796th epoch : [0.72055138]  Training Accuracy:0.500542888165038\n",
      "The training loss at 797th epoch : [0.72049231]  Training Accuracy:0.500542888165038\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training loss at 798th epoch : [0.72043336]  Training Accuracy:0.500542888165038\n",
      "The training loss at 799th epoch : [0.72037454]  Training Accuracy:0.500542888165038\n",
      "The training loss at 800th epoch : [0.72031585]  Training Accuracy:0.500542888165038\n",
      "The training loss at 801th epoch : [0.72025728]  Training Accuracy:0.500542888165038\n",
      "The training loss at 802th epoch : [0.72019883]  Training Accuracy:0.500542888165038\n",
      "The training loss at 803th epoch : [0.72014051]  Training Accuracy:0.500542888165038\n",
      "The training loss at 804th epoch : [0.72008232]  Training Accuracy:0.500542888165038\n",
      "The training loss at 805th epoch : [0.72002424]  Training Accuracy:0.500542888165038\n",
      "The training loss at 806th epoch : [0.71996629]  Training Accuracy:0.500542888165038\n",
      "The training loss at 807th epoch : [0.71990847]  Training Accuracy:0.500542888165038\n",
      "The training loss at 808th epoch : [0.71985076]  Training Accuracy:0.500542888165038\n",
      "The training loss at 809th epoch : [0.71979318]  Training Accuracy:0.500542888165038\n",
      "The training loss at 810th epoch : [0.71973573]  Training Accuracy:0.500542888165038\n",
      "The training loss at 811th epoch : [0.71967839]  Training Accuracy:0.500542888165038\n",
      "The training loss at 812th epoch : [0.71962118]  Training Accuracy:0.500542888165038\n",
      "The training loss at 813th epoch : [0.71956409]  Training Accuracy:0.500542888165038\n",
      "The training loss at 814th epoch : [0.71950711]  Training Accuracy:0.500542888165038\n",
      "The training loss at 815th epoch : [0.71945027]  Training Accuracy:0.500542888165038\n",
      "The training loss at 816th epoch : [0.71939354]  Training Accuracy:0.500542888165038\n",
      "The training loss at 817th epoch : [0.71933693]  Training Accuracy:0.500542888165038\n",
      "The training loss at 818th epoch : [0.71928044]  Training Accuracy:0.500542888165038\n",
      "The training loss at 819th epoch : [0.71922408]  Training Accuracy:0.500542888165038\n",
      "The training loss at 820th epoch : [0.71916783]  Training Accuracy:0.500542888165038\n",
      "The training loss at 821th epoch : [0.7191117]  Training Accuracy:0.500542888165038\n",
      "The training loss at 822th epoch : [0.7190557]  Training Accuracy:0.500542888165038\n",
      "The training loss at 823th epoch : [0.71899981]  Training Accuracy:0.500542888165038\n",
      "The training loss at 824th epoch : [0.71894404]  Training Accuracy:0.500542888165038\n",
      "The training loss at 825th epoch : [0.71888839]  Training Accuracy:0.500542888165038\n",
      "The training loss at 826th epoch : [0.71883286]  Training Accuracy:0.500542888165038\n",
      "The training loss at 827th epoch : [0.71877745]  Training Accuracy:0.500542888165038\n",
      "The training loss at 828th epoch : [0.71872215]  Training Accuracy:0.500542888165038\n",
      "The training loss at 829th epoch : [0.71866697]  Training Accuracy:0.500542888165038\n",
      "The training loss at 830th epoch : [0.71861191]  Training Accuracy:0.500542888165038\n",
      "The training loss at 831th epoch : [0.71855697]  Training Accuracy:0.500542888165038\n",
      "The training loss at 832th epoch : [0.71850215]  Training Accuracy:0.500542888165038\n",
      "The training loss at 833th epoch : [0.71844744]  Training Accuracy:0.500542888165038\n",
      "The training loss at 834th epoch : [0.71839285]  Training Accuracy:0.500542888165038\n",
      "The training loss at 835th epoch : [0.71833837]  Training Accuracy:0.500542888165038\n",
      "The training loss at 836th epoch : [0.71828401]  Training Accuracy:0.500542888165038\n",
      "The training loss at 837th epoch : [0.71822977]  Training Accuracy:0.500542888165038\n",
      "The training loss at 838th epoch : [0.71817564]  Training Accuracy:0.500542888165038\n",
      "The training loss at 839th epoch : [0.71812163]  Training Accuracy:0.500542888165038\n",
      "The training loss at 840th epoch : [0.71806773]  Training Accuracy:0.500542888165038\n",
      "The training loss at 841th epoch : [0.71801395]  Training Accuracy:0.500542888165038\n",
      "The training loss at 842th epoch : [0.71796028]  Training Accuracy:0.500542888165038\n",
      "The training loss at 843th epoch : [0.71790673]  Training Accuracy:0.500542888165038\n",
      "The training loss at 844th epoch : [0.71785329]  Training Accuracy:0.500542888165038\n",
      "The training loss at 845th epoch : [0.71779996]  Training Accuracy:0.500542888165038\n",
      "The training loss at 846th epoch : [0.71774675]  Training Accuracy:0.500542888165038\n",
      "The training loss at 847th epoch : [0.71769365]  Training Accuracy:0.500542888165038\n",
      "The training loss at 848th epoch : [0.71764067]  Training Accuracy:0.500542888165038\n",
      "The training loss at 849th epoch : [0.7175878]  Training Accuracy:0.500542888165038\n",
      "The training loss at 850th epoch : [0.71753504]  Training Accuracy:0.500542888165038\n",
      "The training loss at 851th epoch : [0.71748239]  Training Accuracy:0.500542888165038\n",
      "The training loss at 852th epoch : [0.71742986]  Training Accuracy:0.500542888165038\n",
      "The training loss at 853th epoch : [0.71737744]  Training Accuracy:0.500542888165038\n",
      "The training loss at 854th epoch : [0.71732513]  Training Accuracy:0.500542888165038\n",
      "The training loss at 855th epoch : [0.71727293]  Training Accuracy:0.500542888165038\n",
      "The training loss at 856th epoch : [0.71722085]  Training Accuracy:0.500542888165038\n",
      "The training loss at 857th epoch : [0.71716887]  Training Accuracy:0.500542888165038\n",
      "The training loss at 858th epoch : [0.71711701]  Training Accuracy:0.500542888165038\n",
      "The training loss at 859th epoch : [0.71706525]  Training Accuracy:0.500542888165038\n",
      "The training loss at 860th epoch : [0.71701361]  Training Accuracy:0.500542888165038\n",
      "The training loss at 861th epoch : [0.71696208]  Training Accuracy:0.500542888165038\n",
      "The training loss at 862th epoch : [0.71691065]  Training Accuracy:0.500542888165038\n",
      "The training loss at 863th epoch : [0.71685934]  Training Accuracy:0.500542888165038\n",
      "The training loss at 864th epoch : [0.71680814]  Training Accuracy:0.500542888165038\n",
      "The training loss at 865th epoch : [0.71675704]  Training Accuracy:0.500542888165038\n",
      "The training loss at 866th epoch : [0.71670606]  Training Accuracy:0.500542888165038\n",
      "The training loss at 867th epoch : [0.71665518]  Training Accuracy:0.500542888165038\n",
      "The training loss at 868th epoch : [0.71660442]  Training Accuracy:0.500542888165038\n",
      "The training loss at 869th epoch : [0.71655376]  Training Accuracy:0.500542888165038\n",
      "The training loss at 870th epoch : [0.71650321]  Training Accuracy:0.500542888165038\n",
      "The training loss at 871th epoch : [0.71645277]  Training Accuracy:0.500542888165038\n",
      "The training loss at 872th epoch : [0.71640243]  Training Accuracy:0.500542888165038\n",
      "The training loss at 873th epoch : [0.7163522]  Training Accuracy:0.500542888165038\n",
      "The training loss at 874th epoch : [0.71630208]  Training Accuracy:0.500542888165038\n",
      "The training loss at 875th epoch : [0.71625207]  Training Accuracy:0.500542888165038\n",
      "The training loss at 876th epoch : [0.71620217]  Training Accuracy:0.500542888165038\n",
      "The training loss at 877th epoch : [0.71615237]  Training Accuracy:0.500542888165038\n",
      "The training loss at 878th epoch : [0.71610267]  Training Accuracy:0.500542888165038\n",
      "The training loss at 879th epoch : [0.71605309]  Training Accuracy:0.500542888165038\n",
      "The training loss at 880th epoch : [0.71600361]  Training Accuracy:0.500542888165038\n",
      "The training loss at 881th epoch : [0.71595423]  Training Accuracy:0.500542888165038\n",
      "The training loss at 882th epoch : [0.71590496]  Training Accuracy:0.500542888165038\n",
      "The training loss at 883th epoch : [0.7158558]  Training Accuracy:0.500542888165038\n",
      "The training loss at 884th epoch : [0.71580674]  Training Accuracy:0.500542888165038\n",
      "The training loss at 885th epoch : [0.71575779]  Training Accuracy:0.500542888165038\n",
      "The training loss at 886th epoch : [0.71570894]  Training Accuracy:0.500542888165038\n",
      "The training loss at 887th epoch : [0.71566019]  Training Accuracy:0.500542888165038\n",
      "The training loss at 888th epoch : [0.71561155]  Training Accuracy:0.500542888165038\n",
      "The training loss at 889th epoch : [0.71556302]  Training Accuracy:0.500542888165038\n",
      "The training loss at 890th epoch : [0.71551459]  Training Accuracy:0.500542888165038\n",
      "The training loss at 891th epoch : [0.71546626]  Training Accuracy:0.500542888165038\n",
      "The training loss at 892th epoch : [0.71541803]  Training Accuracy:0.500542888165038\n",
      "The training loss at 893th epoch : [0.71536991]  Training Accuracy:0.500542888165038\n",
      "The training loss at 894th epoch : [0.71532189]  Training Accuracy:0.500542888165038\n",
      "The training loss at 895th epoch : [0.71527397]  Training Accuracy:0.500542888165038\n",
      "The training loss at 896th epoch : [0.71522616]  Training Accuracy:0.500542888165038\n",
      "The training loss at 897th epoch : [0.71517845]  Training Accuracy:0.500542888165038\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training loss at 898th epoch : [0.71513084]  Training Accuracy:0.500542888165038\n",
      "The training loss at 899th epoch : [0.71508333]  Training Accuracy:0.500542888165038\n",
      "The training loss at 900th epoch : [0.71503593]  Training Accuracy:0.500542888165038\n",
      "The training loss at 901th epoch : [0.71498862]  Training Accuracy:0.500542888165038\n",
      "The training loss at 902th epoch : [0.71494142]  Training Accuracy:0.500542888165038\n",
      "The training loss at 903th epoch : [0.71489432]  Training Accuracy:0.500542888165038\n",
      "The training loss at 904th epoch : [0.71484732]  Training Accuracy:0.500542888165038\n",
      "The training loss at 905th epoch : [0.71480042]  Training Accuracy:0.500542888165038\n",
      "The training loss at 906th epoch : [0.71475362]  Training Accuracy:0.500542888165038\n",
      "The training loss at 907th epoch : [0.71470692]  Training Accuracy:0.500542888165038\n",
      "The training loss at 908th epoch : [0.71466032]  Training Accuracy:0.500542888165038\n",
      "The training loss at 909th epoch : [0.71461382]  Training Accuracy:0.500542888165038\n",
      "The training loss at 910th epoch : [0.71456742]  Training Accuracy:0.500542888165038\n",
      "The training loss at 911th epoch : [0.71452112]  Training Accuracy:0.500542888165038\n",
      "The training loss at 912th epoch : [0.71447491]  Training Accuracy:0.500542888165038\n",
      "The training loss at 913th epoch : [0.71442881]  Training Accuracy:0.500542888165038\n",
      "The training loss at 914th epoch : [0.71438281]  Training Accuracy:0.500542888165038\n",
      "The training loss at 915th epoch : [0.7143369]  Training Accuracy:0.500542888165038\n",
      "The training loss at 916th epoch : [0.7142911]  Training Accuracy:0.500542888165038\n",
      "The training loss at 917th epoch : [0.71424539]  Training Accuracy:0.500542888165038\n",
      "The training loss at 918th epoch : [0.71419978]  Training Accuracy:0.500542888165038\n",
      "The training loss at 919th epoch : [0.71415426]  Training Accuracy:0.500542888165038\n",
      "The training loss at 920th epoch : [0.71410885]  Training Accuracy:0.500542888165038\n",
      "The training loss at 921th epoch : [0.71406353]  Training Accuracy:0.500542888165038\n",
      "The training loss at 922th epoch : [0.71401831]  Training Accuracy:0.500542888165038\n",
      "The training loss at 923th epoch : [0.71397318]  Training Accuracy:0.500542888165038\n",
      "The training loss at 924th epoch : [0.71392816]  Training Accuracy:0.500542888165038\n",
      "The training loss at 925th epoch : [0.71388322]  Training Accuracy:0.500542888165038\n",
      "The training loss at 926th epoch : [0.71383839]  Training Accuracy:0.500542888165038\n",
      "The training loss at 927th epoch : [0.71379365]  Training Accuracy:0.500542888165038\n",
      "The training loss at 928th epoch : [0.71374901]  Training Accuracy:0.500542888165038\n",
      "The training loss at 929th epoch : [0.71370446]  Training Accuracy:0.500542888165038\n",
      "The training loss at 930th epoch : [0.71366001]  Training Accuracy:0.500542888165038\n",
      "The training loss at 931th epoch : [0.71361565]  Training Accuracy:0.500542888165038\n",
      "The training loss at 932th epoch : [0.71357139]  Training Accuracy:0.500542888165038\n",
      "The training loss at 933th epoch : [0.71352723]  Training Accuracy:0.500542888165038\n",
      "The training loss at 934th epoch : [0.71348316]  Training Accuracy:0.500542888165038\n",
      "The training loss at 935th epoch : [0.71343918]  Training Accuracy:0.500542888165038\n",
      "The training loss at 936th epoch : [0.7133953]  Training Accuracy:0.500542888165038\n",
      "The training loss at 937th epoch : [0.71335151]  Training Accuracy:0.500542888165038\n",
      "The training loss at 938th epoch : [0.71330781]  Training Accuracy:0.500542888165038\n",
      "The training loss at 939th epoch : [0.71326421]  Training Accuracy:0.500542888165038\n",
      "The training loss at 940th epoch : [0.71322071]  Training Accuracy:0.500542888165038\n",
      "The training loss at 941th epoch : [0.71317729]  Training Accuracy:0.500542888165038\n",
      "The training loss at 942th epoch : [0.71313397]  Training Accuracy:0.500542888165038\n",
      "The training loss at 943th epoch : [0.71309074]  Training Accuracy:0.500542888165038\n",
      "The training loss at 944th epoch : [0.71304761]  Training Accuracy:0.500542888165038\n",
      "The training loss at 945th epoch : [0.71300457]  Training Accuracy:0.500542888165038\n",
      "The training loss at 946th epoch : [0.71296162]  Training Accuracy:0.500542888165038\n",
      "The training loss at 947th epoch : [0.71291876]  Training Accuracy:0.500542888165038\n",
      "The training loss at 948th epoch : [0.71287599]  Training Accuracy:0.500542888165038\n",
      "The training loss at 949th epoch : [0.71283332]  Training Accuracy:0.500542888165038\n",
      "The training loss at 950th epoch : [0.71279074]  Training Accuracy:0.500542888165038\n",
      "The training loss at 951th epoch : [0.71274825]  Training Accuracy:0.500542888165038\n",
      "The training loss at 952th epoch : [0.71270585]  Training Accuracy:0.500542888165038\n",
      "The training loss at 953th epoch : [0.71266354]  Training Accuracy:0.500542888165038\n",
      "The training loss at 954th epoch : [0.71262132]  Training Accuracy:0.500542888165038\n",
      "The training loss at 955th epoch : [0.71257919]  Training Accuracy:0.500542888165038\n",
      "The training loss at 956th epoch : [0.71253715]  Training Accuracy:0.500542888165038\n",
      "The training loss at 957th epoch : [0.71249521]  Training Accuracy:0.500542888165038\n",
      "The training loss at 958th epoch : [0.71245335]  Training Accuracy:0.500542888165038\n",
      "The training loss at 959th epoch : [0.71241159]  Training Accuracy:0.500542888165038\n",
      "The training loss at 960th epoch : [0.71236991]  Training Accuracy:0.500542888165038\n",
      "The training loss at 961th epoch : [0.71232832]  Training Accuracy:0.500542888165038\n",
      "The training loss at 962th epoch : [0.71228682]  Training Accuracy:0.500542888165038\n",
      "The training loss at 963th epoch : [0.71224541]  Training Accuracy:0.500542888165038\n",
      "The training loss at 964th epoch : [0.71220409]  Training Accuracy:0.500542888165038\n",
      "The training loss at 965th epoch : [0.71216286]  Training Accuracy:0.500542888165038\n",
      "The training loss at 966th epoch : [0.71212172]  Training Accuracy:0.500542888165038\n",
      "The training loss at 967th epoch : [0.71208067]  Training Accuracy:0.500542888165038\n",
      "The training loss at 968th epoch : [0.7120397]  Training Accuracy:0.500542888165038\n",
      "The training loss at 969th epoch : [0.71199882]  Training Accuracy:0.500542888165038\n",
      "The training loss at 970th epoch : [0.71195803]  Training Accuracy:0.500542888165038\n",
      "The training loss at 971th epoch : [0.71191733]  Training Accuracy:0.500542888165038\n",
      "The training loss at 972th epoch : [0.71187672]  Training Accuracy:0.500542888165038\n",
      "The training loss at 973th epoch : [0.71183619]  Training Accuracy:0.500542888165038\n",
      "The training loss at 974th epoch : [0.71179575]  Training Accuracy:0.500542888165038\n",
      "The training loss at 975th epoch : [0.71175539]  Training Accuracy:0.500542888165038\n",
      "The training loss at 976th epoch : [0.71171513]  Training Accuracy:0.500542888165038\n",
      "The training loss at 977th epoch : [0.71167495]  Training Accuracy:0.500542888165038\n",
      "The training loss at 978th epoch : [0.71163485]  Training Accuracy:0.500542888165038\n",
      "The training loss at 979th epoch : [0.71159485]  Training Accuracy:0.500542888165038\n",
      "The training loss at 980th epoch : [0.71155492]  Training Accuracy:0.500542888165038\n",
      "The training loss at 981th epoch : [0.71151509]  Training Accuracy:0.500542888165038\n",
      "The training loss at 982th epoch : [0.71147534]  Training Accuracy:0.500542888165038\n",
      "The training loss at 983th epoch : [0.71143567]  Training Accuracy:0.500542888165038\n",
      "The training loss at 984th epoch : [0.7113961]  Training Accuracy:0.500542888165038\n",
      "The training loss at 985th epoch : [0.7113566]  Training Accuracy:0.500542888165038\n",
      "The training loss at 986th epoch : [0.71131719]  Training Accuracy:0.500542888165038\n",
      "The training loss at 987th epoch : [0.71127787]  Training Accuracy:0.500542888165038\n",
      "The training loss at 988th epoch : [0.71123863]  Training Accuracy:0.500542888165038\n",
      "The training loss at 989th epoch : [0.71119947]  Training Accuracy:0.500542888165038\n",
      "The training loss at 990th epoch : [0.7111604]  Training Accuracy:0.500542888165038\n",
      "The training loss at 991th epoch : [0.71112141]  Training Accuracy:0.500542888165038\n",
      "The training loss at 992th epoch : [0.71108251]  Training Accuracy:0.500542888165038\n",
      "The training loss at 993th epoch : [0.71104369]  Training Accuracy:0.500542888165038\n",
      "The training loss at 994th epoch : [0.71100495]  Training Accuracy:0.500542888165038\n",
      "The training loss at 995th epoch : [0.7109663]  Training Accuracy:0.500542888165038\n",
      "The training loss at 996th epoch : [0.71092773]  Training Accuracy:0.500542888165038\n",
      "The training loss at 997th epoch : [0.71088925]  Training Accuracy:0.500542888165038\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training loss at 998th epoch : [0.71085084]  Training Accuracy:0.500542888165038\n",
      "The training loss at 999th epoch : [0.71081252]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1000th epoch : [0.71077428]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1001th epoch : [0.71073613]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1002th epoch : [0.71069805]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1003th epoch : [0.71066006]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1004th epoch : [0.71062215]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1005th epoch : [0.71058432]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1006th epoch : [0.71054657]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1007th epoch : [0.71050891]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1008th epoch : [0.71047132]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1009th epoch : [0.71043382]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1010th epoch : [0.71039639]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1011th epoch : [0.71035905]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1012th epoch : [0.71032179]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1013th epoch : [0.71028461]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1014th epoch : [0.71024751]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1015th epoch : [0.71021048]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1016th epoch : [0.71017354]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1017th epoch : [0.71013668]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1018th epoch : [0.71009989]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1019th epoch : [0.71006319]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1020th epoch : [0.71002656]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1021th epoch : [0.70999002]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1022th epoch : [0.70995355]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1023th epoch : [0.70991716]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1024th epoch : [0.70988085]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1025th epoch : [0.70984462]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1026th epoch : [0.70980847]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1027th epoch : [0.70977239]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1028th epoch : [0.70973639]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1029th epoch : [0.70970047]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1030th epoch : [0.70966463]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1031th epoch : [0.70962886]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1032th epoch : [0.70959317]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1033th epoch : [0.70955756]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1034th epoch : [0.70952202]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1035th epoch : [0.70948656]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1036th epoch : [0.70945118]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1037th epoch : [0.70941588]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1038th epoch : [0.70938065]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1039th epoch : [0.70934549]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1040th epoch : [0.70931041]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1041th epoch : [0.70927541]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1042th epoch : [0.70924048]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1043th epoch : [0.70920563]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1044th epoch : [0.70917086]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1045th epoch : [0.70913615]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1046th epoch : [0.70910153]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1047th epoch : [0.70906697]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1048th epoch : [0.7090325]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1049th epoch : [0.70899809]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1050th epoch : [0.70896376]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1051th epoch : [0.70892951]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1052th epoch : [0.70889533]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1053th epoch : [0.70886122]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1054th epoch : [0.70882719]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1055th epoch : [0.70879323]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1056th epoch : [0.70875934]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1057th epoch : [0.70872553]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1058th epoch : [0.70869178]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1059th epoch : [0.70865812]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1060th epoch : [0.70862452]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1061th epoch : [0.708591]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1062th epoch : [0.70855755]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1063th epoch : [0.70852417]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1064th epoch : [0.70849086]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1065th epoch : [0.70845763]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1066th epoch : [0.70842446]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1067th epoch : [0.70839137]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1068th epoch : [0.70835835]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1069th epoch : [0.7083254]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1070th epoch : [0.70829253]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1071th epoch : [0.70825972]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1072th epoch : [0.70822698]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1073th epoch : [0.70819432]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1074th epoch : [0.70816172]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1075th epoch : [0.7081292]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1076th epoch : [0.70809675]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1077th epoch : [0.70806436]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1078th epoch : [0.70803205]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1079th epoch : [0.70799981]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1080th epoch : [0.70796763]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1081th epoch : [0.70793553]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1082th epoch : [0.70790349]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1083th epoch : [0.70787152]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1084th epoch : [0.70783963]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1085th epoch : [0.7078078]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1086th epoch : [0.70777604]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1087th epoch : [0.70774435]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1088th epoch : [0.70771272]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1089th epoch : [0.70768117]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1090th epoch : [0.70764968]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1091th epoch : [0.70761827]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1092th epoch : [0.70758692]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1093th epoch : [0.70755563]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1094th epoch : [0.70752442]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1095th epoch : [0.70749327]  Training Accuracy:0.500542888165038\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training loss at 1096th epoch : [0.70746219]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1097th epoch : [0.70743118]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1098th epoch : [0.70740023]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1099th epoch : [0.70736935]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1100th epoch : [0.70733854]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1101th epoch : [0.70730779]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1102th epoch : [0.70727712]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1103th epoch : [0.7072465]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1104th epoch : [0.70721596]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1105th epoch : [0.70718548]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1106th epoch : [0.70715506]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1107th epoch : [0.70712471]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1108th epoch : [0.70709443]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1109th epoch : [0.70706421]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1110th epoch : [0.70703406]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1111th epoch : [0.70700397]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1112th epoch : [0.70697395]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1113th epoch : [0.70694399]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1114th epoch : [0.7069141]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1115th epoch : [0.70688427]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1116th epoch : [0.70685451]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1117th epoch : [0.70682481]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1118th epoch : [0.70679517]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1119th epoch : [0.7067656]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1120th epoch : [0.70673609]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1121th epoch : [0.70670665]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1122th epoch : [0.70667727]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1123th epoch : [0.70664795]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1124th epoch : [0.7066187]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1125th epoch : [0.70658951]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1126th epoch : [0.70656039]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1127th epoch : [0.70653132]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1128th epoch : [0.70650232]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1129th epoch : [0.70647339]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1130th epoch : [0.70644451]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1131th epoch : [0.7064157]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1132th epoch : [0.70638695]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1133th epoch : [0.70635826]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1134th epoch : [0.70632964]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1135th epoch : [0.70630107]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1136th epoch : [0.70627257]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1137th epoch : [0.70624413]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1138th epoch : [0.70621575]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1139th epoch : [0.70618743]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1140th epoch : [0.70615918]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1141th epoch : [0.70613098]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1142th epoch : [0.70610285]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1143th epoch : [0.70607478]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1144th epoch : [0.70604677]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1145th epoch : [0.70601881]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1146th epoch : [0.70599092]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1147th epoch : [0.70596309]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1148th epoch : [0.70593532]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1149th epoch : [0.70590761]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1150th epoch : [0.70587996]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1151th epoch : [0.70585237]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1152th epoch : [0.70582484]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1153th epoch : [0.70579737]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1154th epoch : [0.70576996]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1155th epoch : [0.70574261]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1156th epoch : [0.70571531]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1157th epoch : [0.70568808]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1158th epoch : [0.70566091]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1159th epoch : [0.70563379]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1160th epoch : [0.70560673]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1161th epoch : [0.70557973]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1162th epoch : [0.70555279]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1163th epoch : [0.70552591]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1164th epoch : [0.70549909]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1165th epoch : [0.70547232]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1166th epoch : [0.70544562]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1167th epoch : [0.70541897]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1168th epoch : [0.70539237]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1169th epoch : [0.70536584]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1170th epoch : [0.70533936]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1171th epoch : [0.70531294]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1172th epoch : [0.70528658]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1173th epoch : [0.70526028]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1174th epoch : [0.70523403]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1175th epoch : [0.70520784]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1176th epoch : [0.7051817]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1177th epoch : [0.70515563]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1178th epoch : [0.70512961]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1179th epoch : [0.70510364]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1180th epoch : [0.70507773]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1181th epoch : [0.70505188]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1182th epoch : [0.70502608]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1183th epoch : [0.70500034]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1184th epoch : [0.70497466]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1185th epoch : [0.70494903]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1186th epoch : [0.70492346]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1187th epoch : [0.70489794]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1188th epoch : [0.70487248]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1189th epoch : [0.70484707]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1190th epoch : [0.70482172]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1191th epoch : [0.70479642]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1192th epoch : [0.70477118]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1193th epoch : [0.70474599]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1194th epoch : [0.70472085]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1195th epoch : [0.70469578]  Training Accuracy:0.500542888165038\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training loss at 1196th epoch : [0.70467075]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1197th epoch : [0.70464578]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1198th epoch : [0.70462087]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1199th epoch : [0.704596]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1200th epoch : [0.7045712]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1201th epoch : [0.70454644]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1202th epoch : [0.70452174]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1203th epoch : [0.70449709]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1204th epoch : [0.7044725]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1205th epoch : [0.70444796]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1206th epoch : [0.70442347]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1207th epoch : [0.70439904]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1208th epoch : [0.70437466]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1209th epoch : [0.70435033]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1210th epoch : [0.70432606]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1211th epoch : [0.70430183]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1212th epoch : [0.70427766]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1213th epoch : [0.70425355]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1214th epoch : [0.70422948]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1215th epoch : [0.70420547]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1216th epoch : [0.70418151]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1217th epoch : [0.7041576]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1218th epoch : [0.70413374]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1219th epoch : [0.70410994]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1220th epoch : [0.70408618]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1221th epoch : [0.70406248]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1222th epoch : [0.70403883]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1223th epoch : [0.70401523]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1224th epoch : [0.70399168]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1225th epoch : [0.70396819]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1226th epoch : [0.70394474]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1227th epoch : [0.70392134]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1228th epoch : [0.703898]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1229th epoch : [0.70387471]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1230th epoch : [0.70385146]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1231th epoch : [0.70382827]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1232th epoch : [0.70380513]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1233th epoch : [0.70378204]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1234th epoch : [0.703759]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1235th epoch : [0.703736]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1236th epoch : [0.70371306]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1237th epoch : [0.70369017]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1238th epoch : [0.70366733]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1239th epoch : [0.70364453]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1240th epoch : [0.70362179]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1241th epoch : [0.7035991]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1242th epoch : [0.70357645]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1243th epoch : [0.70355386]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1244th epoch : [0.70353131]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1245th epoch : [0.70350881]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1246th epoch : [0.70348636]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1247th epoch : [0.70346396]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1248th epoch : [0.70344161]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1249th epoch : [0.70341931]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1250th epoch : [0.70339706]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1251th epoch : [0.70337485]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1252th epoch : [0.70335269]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1253th epoch : [0.70333058]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1254th epoch : [0.70330852]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1255th epoch : [0.70328651]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1256th epoch : [0.70326454]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1257th epoch : [0.70324262]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1258th epoch : [0.70322075]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1259th epoch : [0.70319893]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1260th epoch : [0.70317715]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1261th epoch : [0.70315542]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1262th epoch : [0.70313374]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1263th epoch : [0.70311211]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1264th epoch : [0.70309052]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1265th epoch : [0.70306898]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1266th epoch : [0.70304749]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1267th epoch : [0.70302604]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1268th epoch : [0.70300464]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1269th epoch : [0.70298329]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1270th epoch : [0.70296198]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1271th epoch : [0.70294072]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1272th epoch : [0.70291951]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1273th epoch : [0.70289834]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1274th epoch : [0.70287721]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1275th epoch : [0.70285614]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1276th epoch : [0.70283511]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1277th epoch : [0.70281412]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1278th epoch : [0.70279318]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1279th epoch : [0.70277229]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1280th epoch : [0.70275144]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1281th epoch : [0.70273064]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1282th epoch : [0.70270988]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1283th epoch : [0.70268917]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1284th epoch : [0.7026685]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1285th epoch : [0.70264787]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1286th epoch : [0.7026273]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1287th epoch : [0.70260676]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1288th epoch : [0.70258627]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1289th epoch : [0.70256583]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1290th epoch : [0.70254543]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1291th epoch : [0.70252507]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1292th epoch : [0.70250476]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1293th epoch : [0.7024845]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1294th epoch : [0.70246427]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1295th epoch : [0.70244409]  Training Accuracy:0.500542888165038\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training loss at 1296th epoch : [0.70242396]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1297th epoch : [0.70240387]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1298th epoch : [0.70238382]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1299th epoch : [0.70236382]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1300th epoch : [0.70234385]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1301th epoch : [0.70232394]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1302th epoch : [0.70230406]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1303th epoch : [0.70228423]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1304th epoch : [0.70226445]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1305th epoch : [0.7022447]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1306th epoch : [0.702225]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1307th epoch : [0.70220534]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1308th epoch : [0.70218572]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1309th epoch : [0.70216615]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1310th epoch : [0.70214662]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1311th epoch : [0.70212713]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1312th epoch : [0.70210769]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1313th epoch : [0.70208828]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1314th epoch : [0.70206892]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1315th epoch : [0.7020496]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1316th epoch : [0.70203032]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1317th epoch : [0.70201109]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1318th epoch : [0.7019919]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1319th epoch : [0.70197274]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1320th epoch : [0.70195363]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1321th epoch : [0.70193456]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1322th epoch : [0.70191554]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1323th epoch : [0.70189655]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1324th epoch : [0.70187761]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1325th epoch : [0.7018587]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1326th epoch : [0.70183984]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1327th epoch : [0.70182102]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1328th epoch : [0.70180224]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1329th epoch : [0.7017835]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1330th epoch : [0.7017648]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1331th epoch : [0.70174615]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1332th epoch : [0.70172753]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1333th epoch : [0.70170895]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1334th epoch : [0.70169042]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1335th epoch : [0.70167192]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1336th epoch : [0.70165347]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1337th epoch : [0.70163505]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1338th epoch : [0.70161667]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1339th epoch : [0.70159834]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1340th epoch : [0.70158004]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1341th epoch : [0.70156179]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1342th epoch : [0.70154357]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1343th epoch : [0.7015254]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1344th epoch : [0.70150726]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1345th epoch : [0.70148916]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1346th epoch : [0.7014711]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1347th epoch : [0.70145309]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1348th epoch : [0.70143511]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1349th epoch : [0.70141717]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1350th epoch : [0.70139927]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1351th epoch : [0.7013814]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1352th epoch : [0.70136358]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1353th epoch : [0.7013458]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1354th epoch : [0.70132805]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1355th epoch : [0.70131034]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1356th epoch : [0.70129267]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1357th epoch : [0.70127504]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1358th epoch : [0.70125745]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1359th epoch : [0.7012399]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1360th epoch : [0.70122238]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1361th epoch : [0.70120491]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1362th epoch : [0.70118747]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1363th epoch : [0.70117007]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1364th epoch : [0.7011527]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1365th epoch : [0.70113538]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1366th epoch : [0.70111809]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1367th epoch : [0.70110084]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1368th epoch : [0.70108363]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1369th epoch : [0.70106645]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1370th epoch : [0.70104932]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1371th epoch : [0.70103222]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1372th epoch : [0.70101515]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1373th epoch : [0.70099813]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1374th epoch : [0.70098114]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1375th epoch : [0.70096419]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1376th epoch : [0.70094727]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1377th epoch : [0.70093039]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1378th epoch : [0.70091355]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1379th epoch : [0.70089675]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1380th epoch : [0.70087998]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1381th epoch : [0.70086325]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1382th epoch : [0.70084655]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1383th epoch : [0.7008299]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1384th epoch : [0.70081327]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1385th epoch : [0.70079669]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1386th epoch : [0.70078014]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1387th epoch : [0.70076362]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1388th epoch : [0.70074715]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1389th epoch : [0.7007307]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1390th epoch : [0.7007143]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1391th epoch : [0.70069793]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1392th epoch : [0.70068159]  Training Accuracy:0.500542888165038\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training loss at 1393th epoch : [0.70066529]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1394th epoch : [0.70064903]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1395th epoch : [0.7006328]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1396th epoch : [0.70061661]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1397th epoch : [0.70060045]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1398th epoch : [0.70058433]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1399th epoch : [0.70056824]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1400th epoch : [0.70055219]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1401th epoch : [0.70053617]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1402th epoch : [0.70052019]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1403th epoch : [0.70050424]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1404th epoch : [0.70048833]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1405th epoch : [0.70047245]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1406th epoch : [0.70045661]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1407th epoch : [0.7004408]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1408th epoch : [0.70042502]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1409th epoch : [0.70040928]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1410th epoch : [0.70039358]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1411th epoch : [0.70037791]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1412th epoch : [0.70036227]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1413th epoch : [0.70034667]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1414th epoch : [0.7003311]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1415th epoch : [0.70031556]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1416th epoch : [0.70030006]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1417th epoch : [0.70028459]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1418th epoch : [0.70026916]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1419th epoch : [0.70025376]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1420th epoch : [0.70023839]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1421th epoch : [0.70022306]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1422th epoch : [0.70020776]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1423th epoch : [0.70019249]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1424th epoch : [0.70017726]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1425th epoch : [0.70016206]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1426th epoch : [0.70014689]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1427th epoch : [0.70013176]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1428th epoch : [0.70011666]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1429th epoch : [0.70010159]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1430th epoch : [0.70008655]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1431th epoch : [0.70007155]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1432th epoch : [0.70005658]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1433th epoch : [0.70004164]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1434th epoch : [0.70002674]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1435th epoch : [0.70001187]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1436th epoch : [0.69999703]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1437th epoch : [0.69998222]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1438th epoch : [0.69996745]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1439th epoch : [0.6999527]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1440th epoch : [0.69993799]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1441th epoch : [0.69992331]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1442th epoch : [0.69990867]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1443th epoch : [0.69989405]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1444th epoch : [0.69987947]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1445th epoch : [0.69986492]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1446th epoch : [0.6998504]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1447th epoch : [0.69983591]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1448th epoch : [0.69982146]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1449th epoch : [0.69980703]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1450th epoch : [0.69979264]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1451th epoch : [0.69977828]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1452th epoch : [0.69976395]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1453th epoch : [0.69974965]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1454th epoch : [0.69973538]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1455th epoch : [0.69972115]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1456th epoch : [0.69970694]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1457th epoch : [0.69969277]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1458th epoch : [0.69967862]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1459th epoch : [0.69966451]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1460th epoch : [0.69965043]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1461th epoch : [0.69963638]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1462th epoch : [0.69962236]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1463th epoch : [0.69960837]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1464th epoch : [0.69959441]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1465th epoch : [0.69958048]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1466th epoch : [0.69956658]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1467th epoch : [0.69955271]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1468th epoch : [0.69953887]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1469th epoch : [0.69952506]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1470th epoch : [0.69951129]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1471th epoch : [0.69949754]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1472th epoch : [0.69948382]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1473th epoch : [0.69947013]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1474th epoch : [0.69945647]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1475th epoch : [0.69944285]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1476th epoch : [0.69942925]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1477th epoch : [0.69941568]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1478th epoch : [0.69940214]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1479th epoch : [0.69938863]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1480th epoch : [0.69937515]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1481th epoch : [0.6993617]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1482th epoch : [0.69934828]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1483th epoch : [0.69933489]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1484th epoch : [0.69932152]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1485th epoch : [0.69930819]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1486th epoch : [0.69929488]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1487th epoch : [0.69928161]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1488th epoch : [0.69926836]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1489th epoch : [0.69925514]  Training Accuracy:0.500542888165038\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training loss at 1490th epoch : [0.69924196]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1491th epoch : [0.6992288]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1492th epoch : [0.69921566]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1493th epoch : [0.69920256]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1494th epoch : [0.69918949]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1495th epoch : [0.69917644]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1496th epoch : [0.69916342]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1497th epoch : [0.69915044]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1498th epoch : [0.69913748]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1499th epoch : [0.69912454]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1500th epoch : [0.69911164]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1501th epoch : [0.69909876]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1502th epoch : [0.69908592]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1503th epoch : [0.6990731]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1504th epoch : [0.69906031]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1505th epoch : [0.69904754]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1506th epoch : [0.69903481]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1507th epoch : [0.6990221]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1508th epoch : [0.69900942]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1509th epoch : [0.69899676]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1510th epoch : [0.69898414]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1511th epoch : [0.69897154]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1512th epoch : [0.69895897]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1513th epoch : [0.69894643]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1514th epoch : [0.69893392]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1515th epoch : [0.69892143]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1516th epoch : [0.69890897]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1517th epoch : [0.69889653]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1518th epoch : [0.69888413]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1519th epoch : [0.69887175]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1520th epoch : [0.6988594]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1521th epoch : [0.69884707]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1522th epoch : [0.69883477]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1523th epoch : [0.6988225]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1524th epoch : [0.69881026]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1525th epoch : [0.69879804]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1526th epoch : [0.69878585]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1527th epoch : [0.69877368]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1528th epoch : [0.69876155]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1529th epoch : [0.69874943]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1530th epoch : [0.69873735]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1531th epoch : [0.69872529]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1532th epoch : [0.69871326]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1533th epoch : [0.69870125]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1534th epoch : [0.69868927]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1535th epoch : [0.69867732]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1536th epoch : [0.69866539]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1537th epoch : [0.69865349]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1538th epoch : [0.69864161]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1539th epoch : [0.69862977]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1540th epoch : [0.69861794]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1541th epoch : [0.69860614]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1542th epoch : [0.69859437]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1543th epoch : [0.69858263]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1544th epoch : [0.6985709]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1545th epoch : [0.69855921]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1546th epoch : [0.69854754]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1547th epoch : [0.6985359]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1548th epoch : [0.69852428]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1549th epoch : [0.69851268]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1550th epoch : [0.69850112]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1551th epoch : [0.69848957]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1552th epoch : [0.69847806]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1553th epoch : [0.69846656]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1554th epoch : [0.6984551]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1555th epoch : [0.69844365]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1556th epoch : [0.69843224]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1557th epoch : [0.69842084]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1558th epoch : [0.69840948]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1559th epoch : [0.69839813]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1560th epoch : [0.69838682]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1561th epoch : [0.69837552]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1562th epoch : [0.69836425]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1563th epoch : [0.69835301]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1564th epoch : [0.69834179]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1565th epoch : [0.6983306]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1566th epoch : [0.69831943]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1567th epoch : [0.69830828]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1568th epoch : [0.69829716]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1569th epoch : [0.69828606]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1570th epoch : [0.69827499]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1571th epoch : [0.69826394]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1572th epoch : [0.69825292]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1573th epoch : [0.69824192]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1574th epoch : [0.69823094]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1575th epoch : [0.69821999]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1576th epoch : [0.69820906]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1577th epoch : [0.69819815]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1578th epoch : [0.69818727]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1579th epoch : [0.69817641]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1580th epoch : [0.69816558]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1581th epoch : [0.69815477]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1582th epoch : [0.69814399]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1583th epoch : [0.69813322]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1584th epoch : [0.69812248]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1585th epoch : [0.69811177]  Training Accuracy:0.500542888165038\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training loss at 1586th epoch : [0.69810108]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1587th epoch : [0.69809041]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1588th epoch : [0.69807976]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1589th epoch : [0.69806914]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1590th epoch : [0.69805854]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1591th epoch : [0.69804796]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1592th epoch : [0.69803741]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1593th epoch : [0.69802688]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1594th epoch : [0.69801638]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1595th epoch : [0.69800589]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1596th epoch : [0.69799543]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1597th epoch : [0.69798499]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1598th epoch : [0.69797458]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1599th epoch : [0.69796419]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1600th epoch : [0.69795382]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1601th epoch : [0.69794347]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1602th epoch : [0.69793314]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1603th epoch : [0.69792284]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1604th epoch : [0.69791256]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1605th epoch : [0.69790231]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1606th epoch : [0.69789207]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1607th epoch : [0.69788186]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1608th epoch : [0.69787167]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1609th epoch : [0.6978615]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1610th epoch : [0.69785136]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1611th epoch : [0.69784123]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1612th epoch : [0.69783113]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1613th epoch : [0.69782105]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1614th epoch : [0.697811]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1615th epoch : [0.69780096]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1616th epoch : [0.69779095]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1617th epoch : [0.69778096]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1618th epoch : [0.69777099]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1619th epoch : [0.69776104]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1620th epoch : [0.69775112]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1621th epoch : [0.69774121]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1622th epoch : [0.69773133]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1623th epoch : [0.69772147]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1624th epoch : [0.69771163]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1625th epoch : [0.69770181]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1626th epoch : [0.69769202]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1627th epoch : [0.69768224]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1628th epoch : [0.69767249]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1629th epoch : [0.69766276]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1630th epoch : [0.69765304]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1631th epoch : [0.69764336]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1632th epoch : [0.69763369]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1633th epoch : [0.69762404]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1634th epoch : [0.69761441]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1635th epoch : [0.69760481]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1636th epoch : [0.69759522]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1637th epoch : [0.69758566]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1638th epoch : [0.69757612]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1639th epoch : [0.6975666]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1640th epoch : [0.6975571]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1641th epoch : [0.69754762]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1642th epoch : [0.69753816]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1643th epoch : [0.69752872]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1644th epoch : [0.6975193]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1645th epoch : [0.6975099]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1646th epoch : [0.69750053]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1647th epoch : [0.69749117]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1648th epoch : [0.69748184]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1649th epoch : [0.69747252]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1650th epoch : [0.69746323]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1651th epoch : [0.69745395]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1652th epoch : [0.6974447]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1653th epoch : [0.69743546]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1654th epoch : [0.69742625]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1655th epoch : [0.69741705]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1656th epoch : [0.69740788]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1657th epoch : [0.69739873]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1658th epoch : [0.69738959]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1659th epoch : [0.69738048]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1660th epoch : [0.69737139]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1661th epoch : [0.69736231]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1662th epoch : [0.69735326]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1663th epoch : [0.69734422]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1664th epoch : [0.69733521]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1665th epoch : [0.69732622]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1666th epoch : [0.69731724]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1667th epoch : [0.69730829]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1668th epoch : [0.69729935]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1669th epoch : [0.69729043]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1670th epoch : [0.69728154]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1671th epoch : [0.69727266]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1672th epoch : [0.6972638]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1673th epoch : [0.69725496]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1674th epoch : [0.69724614]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1675th epoch : [0.69723734]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1676th epoch : [0.69722856]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1677th epoch : [0.6972198]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1678th epoch : [0.69721106]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1679th epoch : [0.69720234]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1680th epoch : [0.69719363]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1681th epoch : [0.69718495]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1682th epoch : [0.69717628]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1683th epoch : [0.69716763]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1684th epoch : [0.69715901]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1685th epoch : [0.6971504]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1686th epoch : [0.69714181]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1687th epoch : [0.69713323]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1688th epoch : [0.69712468]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1689th epoch : [0.69711615]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1690th epoch : [0.69710763]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1691th epoch : [0.69709913]  Training Accuracy:0.500542888165038\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training loss at 1692th epoch : [0.69709066]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1693th epoch : [0.6970822]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1694th epoch : [0.69707376]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1695th epoch : [0.69706533]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1696th epoch : [0.69705693]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1697th epoch : [0.69704854]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1698th epoch : [0.69704017]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1699th epoch : [0.69703182]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1700th epoch : [0.69702349]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1701th epoch : [0.69701518]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1702th epoch : [0.69700689]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1703th epoch : [0.69699861]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1704th epoch : [0.69699035]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1705th epoch : [0.69698211]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1706th epoch : [0.69697389]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1707th epoch : [0.69696568]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1708th epoch : [0.6969575]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1709th epoch : [0.69694933]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1710th epoch : [0.69694118]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1711th epoch : [0.69693305]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1712th epoch : [0.69692493]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1713th epoch : [0.69691683]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1714th epoch : [0.69690875]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1715th epoch : [0.69690069]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1716th epoch : [0.69689265]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1717th epoch : [0.69688462]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1718th epoch : [0.69687661]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1719th epoch : [0.69686862]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1720th epoch : [0.69686065]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1721th epoch : [0.69685269]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1722th epoch : [0.69684475]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1723th epoch : [0.69683683]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1724th epoch : [0.69682892]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1725th epoch : [0.69682104]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1726th epoch : [0.69681317]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1727th epoch : [0.69680531]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1728th epoch : [0.69679748]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1729th epoch : [0.69678966]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1730th epoch : [0.69678186]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1731th epoch : [0.69677407]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1732th epoch : [0.69676631]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1733th epoch : [0.69675856]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1734th epoch : [0.69675082]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1735th epoch : [0.69674311]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1736th epoch : [0.69673541]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1737th epoch : [0.69672773]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1738th epoch : [0.69672006]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1739th epoch : [0.69671241]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1740th epoch : [0.69670478]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1741th epoch : [0.69669716]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1742th epoch : [0.69668956]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1743th epoch : [0.69668198]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1744th epoch : [0.69667442]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1745th epoch : [0.69666687]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1746th epoch : [0.69665933]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1747th epoch : [0.69665182]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1748th epoch : [0.69664432]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1749th epoch : [0.69663684]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1750th epoch : [0.69662937]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1751th epoch : [0.69662192]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1752th epoch : [0.69661448]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1753th epoch : [0.69660707]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1754th epoch : [0.69659966]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1755th epoch : [0.69659228]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1756th epoch : [0.69658491]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1757th epoch : [0.69657756]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1758th epoch : [0.69657022]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1759th epoch : [0.6965629]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1760th epoch : [0.69655559]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1761th epoch : [0.6965483]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1762th epoch : [0.69654103]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1763th epoch : [0.69653377]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1764th epoch : [0.69652653]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1765th epoch : [0.69651931]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1766th epoch : [0.6965121]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1767th epoch : [0.6965049]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1768th epoch : [0.69649772]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1769th epoch : [0.69649056]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1770th epoch : [0.69648341]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1771th epoch : [0.69647628]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1772th epoch : [0.69646917]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1773th epoch : [0.69646207]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1774th epoch : [0.69645498]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1775th epoch : [0.69644792]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1776th epoch : [0.69644086]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1777th epoch : [0.69643382]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1778th epoch : [0.6964268]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1779th epoch : [0.69641979]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1780th epoch : [0.6964128]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1781th epoch : [0.69640583]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1782th epoch : [0.69639886]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1783th epoch : [0.69639192]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1784th epoch : [0.69638499]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1785th epoch : [0.69637807]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1786th epoch : [0.69637117]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1787th epoch : [0.69636429]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1788th epoch : [0.69635742]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1789th epoch : [0.69635056]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1790th epoch : [0.69634372]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1791th epoch : [0.69633689]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1792th epoch : [0.69633008]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1793th epoch : [0.69632329]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1794th epoch : [0.69631651]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1795th epoch : [0.69630974]  Training Accuracy:0.500542888165038\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training loss at 1796th epoch : [0.69630299]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1797th epoch : [0.69629625]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1798th epoch : [0.69628953]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1799th epoch : [0.69628283]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1800th epoch : [0.69627613]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1801th epoch : [0.69626946]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1802th epoch : [0.69626279]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1803th epoch : [0.69625615]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1804th epoch : [0.69624951]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1805th epoch : [0.69624289]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1806th epoch : [0.69623629]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1807th epoch : [0.6962297]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1808th epoch : [0.69622312]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1809th epoch : [0.69621656]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1810th epoch : [0.69621001]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1811th epoch : [0.69620348]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1812th epoch : [0.69619696]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1813th epoch : [0.69619046]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1814th epoch : [0.69618397]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1815th epoch : [0.69617749]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1816th epoch : [0.69617103]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1817th epoch : [0.69616459]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1818th epoch : [0.69615815]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1819th epoch : [0.69615173]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1820th epoch : [0.69614533]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1821th epoch : [0.69613894]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1822th epoch : [0.69613256]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1823th epoch : [0.6961262]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1824th epoch : [0.69611985]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1825th epoch : [0.69611351]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1826th epoch : [0.69610719]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1827th epoch : [0.69610088]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1828th epoch : [0.69609459]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1829th epoch : [0.69608831]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1830th epoch : [0.69608204]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1831th epoch : [0.69607579]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1832th epoch : [0.69606955]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1833th epoch : [0.69606333]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1834th epoch : [0.69605712]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1835th epoch : [0.69605092]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1836th epoch : [0.69604473]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1837th epoch : [0.69603856]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1838th epoch : [0.69603241]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1839th epoch : [0.69602626]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1840th epoch : [0.69602013]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1841th epoch : [0.69601402]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1842th epoch : [0.69600791]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1843th epoch : [0.69600182]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1844th epoch : [0.69599575]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1845th epoch : [0.69598968]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1846th epoch : [0.69598363]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1847th epoch : [0.6959776]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1848th epoch : [0.69597157]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1849th epoch : [0.69596556]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1850th epoch : [0.69595956]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1851th epoch : [0.69595358]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1852th epoch : [0.69594761]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1853th epoch : [0.69594165]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1854th epoch : [0.69593571]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1855th epoch : [0.69592977]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1856th epoch : [0.69592385]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1857th epoch : [0.69591795]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1858th epoch : [0.69591206]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1859th epoch : [0.69590617]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1860th epoch : [0.69590031]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1861th epoch : [0.69589445]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1862th epoch : [0.69588861]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1863th epoch : [0.69588278]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1864th epoch : [0.69587697]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1865th epoch : [0.69587116]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1866th epoch : [0.69586537]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1867th epoch : [0.69585959]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1868th epoch : [0.69585383]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1869th epoch : [0.69584808]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1870th epoch : [0.69584234]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1871th epoch : [0.69583661]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1872th epoch : [0.69583089]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1873th epoch : [0.69582519]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1874th epoch : [0.6958195]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1875th epoch : [0.69581382]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1876th epoch : [0.69580816]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1877th epoch : [0.69580251]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1878th epoch : [0.69579687]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1879th epoch : [0.69579124]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1880th epoch : [0.69578562]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1881th epoch : [0.69578002]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1882th epoch : [0.69577443]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1883th epoch : [0.69576885]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1884th epoch : [0.69576328]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1885th epoch : [0.69575773]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1886th epoch : [0.69575219]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1887th epoch : [0.69574666]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1888th epoch : [0.69574114]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1889th epoch : [0.69573563]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1890th epoch : [0.69573014]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1891th epoch : [0.69572466]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1892th epoch : [0.69571919]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1893th epoch : [0.69571373]  Training Accuracy:0.500542888165038\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training loss at 1894th epoch : [0.69570828]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1895th epoch : [0.69570285]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1896th epoch : [0.69569743]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1897th epoch : [0.69569202]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1898th epoch : [0.69568662]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1899th epoch : [0.69568123]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1900th epoch : [0.69567586]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1901th epoch : [0.6956705]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1902th epoch : [0.69566515]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1903th epoch : [0.69565981]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1904th epoch : [0.69565448]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1905th epoch : [0.69564916]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1906th epoch : [0.69564386]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1907th epoch : [0.69563856]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1908th epoch : [0.69563328]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1909th epoch : [0.69562801]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1910th epoch : [0.69562276]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1911th epoch : [0.69561751]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1912th epoch : [0.69561227]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1913th epoch : [0.69560705]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1914th epoch : [0.69560184]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1915th epoch : [0.69559664]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1916th epoch : [0.69559145]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1917th epoch : [0.69558627]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1918th epoch : [0.6955811]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1919th epoch : [0.69557595]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1920th epoch : [0.69557081]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1921th epoch : [0.69556567]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1922th epoch : [0.69556055]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1923th epoch : [0.69555544]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1924th epoch : [0.69555034]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1925th epoch : [0.69554525]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1926th epoch : [0.69554018]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1927th epoch : [0.69553511]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1928th epoch : [0.69553006]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1929th epoch : [0.69552501]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1930th epoch : [0.69551998]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1931th epoch : [0.69551496]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1932th epoch : [0.69550995]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1933th epoch : [0.69550495]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1934th epoch : [0.69549996]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1935th epoch : [0.69549498]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1936th epoch : [0.69549002]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1937th epoch : [0.69548506]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1938th epoch : [0.69548012]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1939th epoch : [0.69547518]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1940th epoch : [0.69547026]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1941th epoch : [0.69546535]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1942th epoch : [0.69546045]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1943th epoch : [0.69545555]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1944th epoch : [0.69545067]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1945th epoch : [0.6954458]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1946th epoch : [0.69544095]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1947th epoch : [0.6954361]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1948th epoch : [0.69543126]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1949th epoch : [0.69542643]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1950th epoch : [0.69542162]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1951th epoch : [0.69541681]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1952th epoch : [0.69541202]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1953th epoch : [0.69540723]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1954th epoch : [0.69540246]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1955th epoch : [0.69539769]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1956th epoch : [0.69539294]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1957th epoch : [0.6953882]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1958th epoch : [0.69538346]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1959th epoch : [0.69537874]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1960th epoch : [0.69537403]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1961th epoch : [0.69536933]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1962th epoch : [0.69536464]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1963th epoch : [0.69535996]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1964th epoch : [0.69535528]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1965th epoch : [0.69535062]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1966th epoch : [0.69534597]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1967th epoch : [0.69534133]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1968th epoch : [0.6953367]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1969th epoch : [0.69533208]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1970th epoch : [0.69532747]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1971th epoch : [0.69532287]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1972th epoch : [0.69531829]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1973th epoch : [0.69531371]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1974th epoch : [0.69530914]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1975th epoch : [0.69530458]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1976th epoch : [0.69530003]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1977th epoch : [0.69529549]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1978th epoch : [0.69529096]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1979th epoch : [0.69528644]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1980th epoch : [0.69528193]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1981th epoch : [0.69527743]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1982th epoch : [0.69527294]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1983th epoch : [0.69526846]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1984th epoch : [0.69526399]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1985th epoch : [0.69525953]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1986th epoch : [0.69525508]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1987th epoch : [0.69525064]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1988th epoch : [0.69524621]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1989th epoch : [0.69524178]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1990th epoch : [0.69523737]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1991th epoch : [0.69523297]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1992th epoch : [0.69522858]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1993th epoch : [0.6952242]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1994th epoch : [0.69521982]  Training Accuracy:0.500542888165038\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training loss at 1995th epoch : [0.69521546]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1996th epoch : [0.6952111]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1997th epoch : [0.69520676]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1998th epoch : [0.69520242]  Training Accuracy:0.500542888165038\n",
      "The training loss at 1999th epoch : [0.6951981]  Training Accuracy:0.500542888165038\n"
     ]
    }
   ],
   "source": [
    "train(model,X_train,Y_train , 2000,alpha = 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy:0.500542888165038\n",
      "Test accuracy:0.49514563106796117\n"
     ]
    }
   ],
   "source": [
    "print(\"Training accuracy:{}\".format(accuracy(model,X_train, Y_train)))\n",
    "\n",
    "print(\"Test accuracy:{}\".format(accuracy(model,X_test, Y_test)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def train_recording(model,\n",
    "                    X_train , \n",
    "                    Y_train,\n",
    "                    epochs=200,\n",
    "                    record_at = 100,\n",
    "                    verbose = True,\n",
    "                    learning_rate =0.1,\n",
    "                    learning_rate_decay = False):\n",
    "    train_loss_his = []\n",
    "    train_acc_his = []\n",
    "    test_loss_his = []\n",
    "    test_acc_his = []\n",
    "    epoch_his = []\n",
    "    \n",
    "    for i in range(epochs):\n",
    "        prediction , cache = model.forward(X_train)\n",
    "        loss,d_back= mean_binary_cross_entropy(prediction,Y_train)        \n",
    "        model.update_gradient(cache,d_back,learning_rate)\n",
    "        if learning_rate_decay:\n",
    "            learning_rate *= (1.0 / 1.0 + i)\n",
    "        \n",
    "        \n",
    "        \n",
    "        if i % record_at == 0:\n",
    "            train_loss,_ = mean_binary_cross_entropy(prediction,Y_train)\n",
    "            train_acc = accuracy(model ,X_train,Y_train)\n",
    "            \n",
    "            test_prediction , _ = model.forward(X_test)\n",
    "            test_loss,_ = mean_binary_cross_entropy(test_prediction,Y_test)\n",
    "            test_acc = accuracy(model,X_test,Y_test)\n",
    "            train_loss_his.append(train_loss)\n",
    "            train_acc_his.append(train_acc)\n",
    "            test_loss_his.append(test_loss)\n",
    "            test_acc_his.append(test_acc)\n",
    "            epoch_his.append(i)\n",
    "            \n",
    "            if verbose:\n",
    "                print(\"{}th EPOCH:\\nTraining Loss:{}|Training Accuracy:{}|Test Loss:{}|Test Accuracy:{}\".\\\n",
    "                  format(i , train_loss , train_acc,test_loss,test_acc))\n",
    "    train_loss_his = np.array(train_loss_his).reshape(-1)\n",
    "    train_acc_his = np.array(train_acc_his).reshape(-1)\n",
    "    test_loss_his = np.array(test_loss_his).reshape(-1)\n",
    "    test_acc_his = np.array(test_acc_his).reshape(-1)\n",
    "    epoch_his = np.array(epoch_his).reshape(-1)\n",
    "    return train_loss_his,train_acc_his,test_loss_his,test_acc_his,epoch_his\n",
    "               \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 20) (20,)\n",
      "<function relu at 0x7fe0aa99a620>\n",
      "(20, 20) (20,)\n",
      "<function sigmoid at 0x7fe0aa99a378>\n",
      "(20, 15) (15,)\n",
      "<function sigmoid at 0x7fe0aa99a378>\n",
      "(15, 8) (8,)\n",
      "<function sigmoid at 0x7fe0aa99a378>\n",
      "(8, 4) (4,)\n",
      "<function tanh at 0x7fe0aa99a510>\n",
      "(4, 1) (1,)\n",
      "<function sigmoid at 0x7fe0aa99a378>\n"
     ]
    }
   ],
   "source": [
    "layer_list = [n,20,20,15,8,4,1]\n",
    "activation_list = ['relu','sigmoid','sigmoid','sigmoid','tanh','sigmoid']\n",
    "\n",
    "model = MultiLayerPerceptron(layer_list,activation_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0th EPOCH:\n",
      "Training Loss:[0.76445299]|Training Accuracy:0.500542888165038|Test Loss:[0.75750909]|Test Accuracy:0.49514563106796117\n",
      "10th EPOCH:\n",
      "Training Loss:[0.70033311]|Training Accuracy:0.500542888165038|Test Loss:[0.700906]|Test Accuracy:0.49514563106796117\n",
      "20th EPOCH:\n",
      "Training Loss:[0.69374943]|Training Accuracy:0.501628664495114|Test Loss:[0.69425923]|Test Accuracy:0.49514563106796117\n",
      "30th EPOCH:\n",
      "Training Loss:[0.69326041]|Training Accuracy:0.494028230184582|Test Loss:[0.69344905]|Test Accuracy:0.49514563106796117\n",
      "40th EPOCH:\n",
      "Training Loss:[0.69322615]|Training Accuracy:0.507057546145494|Test Loss:[0.69330757]|Test Accuracy:0.5145631067961165\n",
      "50th EPOCH:\n",
      "Training Loss:[0.69322287]|Training Accuracy:0.510314875135722|Test Loss:[0.69327503]|Test Accuracy:0.5339805825242718\n",
      "60th EPOCH:\n",
      "Training Loss:[0.69322163]|Training Accuracy:0.511400651465798|Test Loss:[0.69326623]|Test Accuracy:0.5145631067961165\n",
      "70th EPOCH:\n",
      "Training Loss:[0.69322052]|Training Accuracy:0.510314875135722|Test Loss:[0.69326327]|Test Accuracy:0.5048543689320388\n",
      "80th EPOCH:\n",
      "Training Loss:[0.69321942]|Training Accuracy:0.509229098805646|Test Loss:[0.69326181]|Test Accuracy:0.5048543689320388\n",
      "90th EPOCH:\n",
      "Training Loss:[0.69321832]|Training Accuracy:0.509229098805646|Test Loss:[0.69326075]|Test Accuracy:0.5145631067961165\n",
      "100th EPOCH:\n",
      "Training Loss:[0.69321724]|Training Accuracy:0.510314875135722|Test Loss:[0.69325982]|Test Accuracy:0.5145631067961165\n",
      "110th EPOCH:\n",
      "Training Loss:[0.69321616]|Training Accuracy:0.510314875135722|Test Loss:[0.69325894]|Test Accuracy:0.5145631067961165\n",
      "120th EPOCH:\n",
      "Training Loss:[0.69321509]|Training Accuracy:0.510314875135722|Test Loss:[0.69325807]|Test Accuracy:0.5145631067961165\n",
      "130th EPOCH:\n",
      "Training Loss:[0.69321404]|Training Accuracy:0.511400651465798|Test Loss:[0.69325723]|Test Accuracy:0.5242718446601942\n",
      "140th EPOCH:\n",
      "Training Loss:[0.69321299]|Training Accuracy:0.511400651465798|Test Loss:[0.69325642]|Test Accuracy:0.5242718446601942\n",
      "150th EPOCH:\n",
      "Training Loss:[0.69321196]|Training Accuracy:0.511400651465798|Test Loss:[0.69325562]|Test Accuracy:0.5242718446601942\n",
      "160th EPOCH:\n",
      "Training Loss:[0.69321093]|Training Accuracy:0.510314875135722|Test Loss:[0.69325483]|Test Accuracy:0.5242718446601942\n",
      "170th EPOCH:\n",
      "Training Loss:[0.69320992]|Training Accuracy:0.510314875135722|Test Loss:[0.69325405]|Test Accuracy:0.5242718446601942\n",
      "180th EPOCH:\n",
      "Training Loss:[0.69320891]|Training Accuracy:0.510314875135722|Test Loss:[0.69325327]|Test Accuracy:0.5242718446601942\n",
      "190th EPOCH:\n",
      "Training Loss:[0.69320791]|Training Accuracy:0.511400651465798|Test Loss:[0.69325251]|Test Accuracy:0.5339805825242718\n",
      "200th EPOCH:\n",
      "Training Loss:[0.69320692]|Training Accuracy:0.510314875135722|Test Loss:[0.69325178]|Test Accuracy:0.5339805825242718\n",
      "210th EPOCH:\n",
      "Training Loss:[0.69320593]|Training Accuracy:0.511400651465798|Test Loss:[0.69325106]|Test Accuracy:0.5339805825242718\n",
      "220th EPOCH:\n",
      "Training Loss:[0.69320496]|Training Accuracy:0.509229098805646|Test Loss:[0.69325035]|Test Accuracy:0.5339805825242718\n",
      "230th EPOCH:\n",
      "Training Loss:[0.69320399]|Training Accuracy:0.509229098805646|Test Loss:[0.69324963]|Test Accuracy:0.5339805825242718\n",
      "240th EPOCH:\n",
      "Training Loss:[0.69320304]|Training Accuracy:0.50814332247557|Test Loss:[0.69324892]|Test Accuracy:0.5339805825242718\n",
      "250th EPOCH:\n",
      "Training Loss:[0.69320209]|Training Accuracy:0.507057546145494|Test Loss:[0.69324822]|Test Accuracy:0.5339805825242718\n",
      "260th EPOCH:\n",
      "Training Loss:[0.69320116]|Training Accuracy:0.507057546145494|Test Loss:[0.69324753]|Test Accuracy:0.5339805825242718\n",
      "270th EPOCH:\n",
      "Training Loss:[0.69320023]|Training Accuracy:0.505971769815418|Test Loss:[0.69324684]|Test Accuracy:0.5339805825242718\n",
      "280th EPOCH:\n",
      "Training Loss:[0.69319931]|Training Accuracy:0.507057546145494|Test Loss:[0.69324618]|Test Accuracy:0.5339805825242718\n",
      "290th EPOCH:\n",
      "Training Loss:[0.69319839]|Training Accuracy:0.507057546145494|Test Loss:[0.69324553]|Test Accuracy:0.5339805825242718\n",
      "300th EPOCH:\n",
      "Training Loss:[0.69319749]|Training Accuracy:0.504885993485342|Test Loss:[0.69324489]|Test Accuracy:0.5339805825242718\n",
      "310th EPOCH:\n",
      "Training Loss:[0.69319659]|Training Accuracy:0.504885993485342|Test Loss:[0.69324427]|Test Accuracy:0.5242718446601942\n",
      "320th EPOCH:\n",
      "Training Loss:[0.6931957]|Training Accuracy:0.504885993485342|Test Loss:[0.69324366]|Test Accuracy:0.5242718446601942\n",
      "330th EPOCH:\n",
      "Training Loss:[0.69319483]|Training Accuracy:0.504885993485342|Test Loss:[0.69324304]|Test Accuracy:0.5242718446601942\n",
      "340th EPOCH:\n",
      "Training Loss:[0.69319396]|Training Accuracy:0.505971769815418|Test Loss:[0.69324242]|Test Accuracy:0.5242718446601942\n",
      "350th EPOCH:\n",
      "Training Loss:[0.6931931]|Training Accuracy:0.507057546145494|Test Loss:[0.69324181]|Test Accuracy:0.5242718446601942\n",
      "360th EPOCH:\n",
      "Training Loss:[0.69319225]|Training Accuracy:0.505971769815418|Test Loss:[0.69324121]|Test Accuracy:0.5242718446601942\n",
      "370th EPOCH:\n",
      "Training Loss:[0.6931914]|Training Accuracy:0.505971769815418|Test Loss:[0.69324062]|Test Accuracy:0.5242718446601942\n",
      "380th EPOCH:\n",
      "Training Loss:[0.69319056]|Training Accuracy:0.504885993485342|Test Loss:[0.69324003]|Test Accuracy:0.5242718446601942\n",
      "390th EPOCH:\n",
      "Training Loss:[0.69318973]|Training Accuracy:0.503800217155266|Test Loss:[0.69323944]|Test Accuracy:0.5145631067961165\n",
      "400th EPOCH:\n",
      "Training Loss:[0.69318891]|Training Accuracy:0.504885993485342|Test Loss:[0.69323883]|Test Accuracy:0.5145631067961165\n",
      "410th EPOCH:\n",
      "Training Loss:[0.69318809]|Training Accuracy:0.504885993485342|Test Loss:[0.69323824]|Test Accuracy:0.5145631067961165\n",
      "420th EPOCH:\n",
      "Training Loss:[0.69318729]|Training Accuracy:0.505971769815418|Test Loss:[0.69323765]|Test Accuracy:0.5145631067961165\n",
      "430th EPOCH:\n",
      "Training Loss:[0.69318649]|Training Accuracy:0.507057546145494|Test Loss:[0.69323707]|Test Accuracy:0.5145631067961165\n",
      "440th EPOCH:\n",
      "Training Loss:[0.69318569]|Training Accuracy:0.504885993485342|Test Loss:[0.69323651]|Test Accuracy:0.5145631067961165\n",
      "450th EPOCH:\n",
      "Training Loss:[0.69318491]|Training Accuracy:0.50814332247557|Test Loss:[0.69323595]|Test Accuracy:0.5145631067961165\n",
      "460th EPOCH:\n",
      "Training Loss:[0.69318413]|Training Accuracy:0.50814332247557|Test Loss:[0.6932354]|Test Accuracy:0.5145631067961165\n",
      "470th EPOCH:\n",
      "Training Loss:[0.69318335]|Training Accuracy:0.507057546145494|Test Loss:[0.69323486]|Test Accuracy:0.5145631067961165\n",
      "480th EPOCH:\n",
      "Training Loss:[0.69318259]|Training Accuracy:0.507057546145494|Test Loss:[0.69323433]|Test Accuracy:0.5145631067961165\n",
      "490th EPOCH:\n",
      "Training Loss:[0.69318184]|Training Accuracy:0.50814332247557|Test Loss:[0.6932338]|Test Accuracy:0.5145631067961165\n"
     ]
    }
   ],
   "source": [
    "train_loss_his,train_acc_his,test_loss_his,test_acc_his,epoch_his = train_recording(model , \n",
    "                                                                                    X_train,\n",
    "                                                                                    Y_train , \n",
    "                                                                                    epochs = 500,\n",
    "                                                                                    record_at = 10,\n",
    "                                                                                    learning_rate= 0.1,\n",
    "                                                                                    learning_rate_decay=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_line, = plt.plot(epoch_his,train_loss_his,label = 'train')\n",
    "test_line, = plt.plot(epoch_his,test_loss_his,label = 'test')\n",
    "plt.xlabel('EPOCHS')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend([train_line, test_line] , ['train','test'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_line, = plt.plot(epoch_his,train_acc_his,label = 'train')\n",
    "test_line, = plt.plot(epoch_his,test_acc_his,label = 'test')\n",
    "plt.xlabel('EPOCHS')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend([train_line, test_line] , ['train','test'])\n",
    "plt.show()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
